---
date: 2025-10-05T09:02:00+02:00
draft: true
params:
  author: J√©r√©my Buget
title: D√©velopper un agent IA custom
---

## Table des mati√®res

- [Introduction](#introduction)
- [TL;DR](#tldr)
- [Objectifs](#objectifs)
- [Cas d'usage](#cas-dusage)
- [Conditions op√©rationnelles et mat√©rielles](#conditions-op√©rationnelles-et-mat√©rielles)
- [Collecte de donn√©es](#collecte-de-donn√©es)
- [Indexation vectorielle](#indexation-vectorielle)
  - [1. Choix du syst√®me de gestion de base de donn√©es (SGBD)](#1-choix-du-syst√®me-de-gestion-de-base-de-donn√©es-sgbd)
  - [2. Cr√©ation de la base + table](#2-cr√©ation-de-la-base--table)
  - [3. Insertion des documents](#3-insertion-des-documents)
    - [3.a) Ollama + `nomic-embed-text:v1.5` = failure ‚ùå](#3a-ollama--nomic-embed-textv15--failure-)
    - [3.b) Sentence Transformer + `nomic-ai/nomic-embed-text-v2-moe` = success ‚úÖ](#3b-sentence-transformer--nomic-ainomic-embed-text-v2-moe--success-)
- [Prompting et g√©n√©ration de r√©ponse](#prompting-et-g√©n√©ration-de-r√©ponse)
- [IHM](#ihm)
- [Conclusion](#conclusion)

## Introduction

Comme tout le monde, j'ai int√©gr√© et continue d'incorporer de plus en plus de pratiques, raisonnements et outils IA dans mon quotidien, pro ou perso : **agents conversationnels** (ChatGPT, Gemini) pour comprendre des choses et g√©n√©rer / am√©liorer du contenu ; **compagnons techniques** (Codex, Claude Code, Zed) pour m'aider √† produire et g√©rer du code ; **algorithmes et mod√®les** (LLM, OCR, STT/TTS) pour r√©aliser des t√¢ches relativement compliqu√©es ou r√©barbatives comme extraire, modifier ou transformer des informations sous toutes les formes.

Savoir utiliser cette technologie et ce qu'elle implique me para√Æt d√©sormais une comp√©tence professionnelle essentielle, quel que soit le m√©tier en question, relatif √† l'informatique ou non. En tant que d√©veloppeur, j'ai cherch√© √† aller un cran plus loin et explor√© comment concevoir et produire un outil exploitant l'IA *dans* et *pour* un domaine fonctionnel particulier.

C'est ainsi que j'ai r√©alis√© un petit chatbot capable d'interroger un corpus pr√©d√©fini de documents sp√©cifiques pour √©laborer des r√©ponses sourc√©es, dans la limite du champs d√©fini.

## TL;DR

- J'ai d√©velopp√© un chatbot IA sp√©cialis√© dans le domaine de l'inclusion socio-professionnelle
- Cet agent se base sur un corpus de documents issus du site [La communaut√© de l'inclusion](https://communaute.inclusion.gouv.fr/)
- J'ai d√©marr√© un serveur Ollama avec le mod√®le open-weight de OpenAI : `gpt-oss:20b`
- J'ai cod√© **un script de crawling** (en Node.js, avec Cheerio.js) pour r√©cup√©rer des fiches d'information
- J'ai install√© **une base de donn√©es PostgreSQL** et activ√© l'extension **pgvector**
- J'ai d√©clar√© une base avec une table disposant d'une colonne de type `embedding VECTOR(768)`
- J'ai vectoris√© ces fiches gr√¢ce √† la lib python **Sentence Transformers** (768 dimensions) et au mod√®le `nomic-ai/nomic-embed-text-v2-moe`
- J'ai index√© les embeddings obtenus dans la base PG
- J'ai d√©velopp√© **une API (Python/FastAPI)** pour mobiliser les diff√©rentes briques IA (query + RAG) sollicit√©es
- Lorsqu'un utilisateur interroge le chatbot, son prompt est √† son tour vectoris√© pour effectuer une recherche par comparaison de vecteurs. On obtient ainsi un ensemble de "documents sources" correspondants (avec un pourcentage de pertinence)
- Je passe alors ces documents comme √©l√©ments exclusifs de "contexte" dans l'instruction que soumets au **LLM responsable de la g√©n√©ration de la r√©ponse** (en interrogeant `/api/chat` de l'API expos√©e par Ollama)
- Je passe aussi au LLM les autres (pr√©c√©dents) messages de la conversation en cours
- J'ai d√©velopp√© **une webapp chatbot-like** toute simple pour poser des questions et afficher les r√©ponses
- Le code source est disponible sur GitHub : [jbuget/ia-custom-chatbot](https://github.com/jbuget/ia-custom-chatbot)

![Capture d'√©cran du chatbot (question + r√©ponse)](chatbot_inclusion_pmsmp-definition.png)


## Objectifs

Mon objectif, √† travers ce POC (proof of concept), √©tait de **comprendre comment exploiter l'IA d'un point de vue v√©ritablement m√©tier, plus seulement comme outil de production ou aide √† la cr√©ation**. Je souhaitais aussi d√©couvrir les solutions envisageables, √©valuer leur potentiel et me confronter √† leurs limites / contraintes.

Concr√®tement, je voulais concevoir une API / webapp capable de d'interagir (√©couter et r√©pondre) en langage naturel, avec des utilisateurs sp√©cifiques d'un domaine, pour r√©pondre le plus pr√©cis√©ment √† des questions m√©tier, en s'appuyant et en citant des sources que j'aurais au pr√©alable collect√©es et index√©es.

## Cas d'usage

J'ai pas mal cherch√© sur quel champs fonctionnel baser le projet.

Ayant beaucoup ≈ìuvr√© dans le service public ces derni√®res ann√©es, j'ai d'abord creus√© du c√¥t√© de [data.gouv.fr](https://data.gouv.fr). S'il y a √©norm√©ment de donn√©es rendues accessibles par les administrations, les collectivit√©s et leurs partenaires, la plupart des datasets sont des donn√©es fortement structur√©es, en colonnes. Pour mon cas d'usage, je souhaitais de la donn√©es moins structur√©es, √† base de contenu et textes riches.

Je me suis tourn√© vers les sites et plateformes de datasets, comme [Kaggle](https://www.kaggle.com/datasets). Mais l√† encore, c'√©tait beaucoup de donn√©es structur√©es, de qualit√© √† g√©om√©trie vraiment variable.

Je pense qu'on retrouve ce type de donn√©es pr√©cieuses en entreprise, et qu'il faut pouvoir les int√©grer et en tenir compte lors de requ√™tes utilisateurs, mais dans le cadre de mon projet d'IA, je voulais vraiment mettre l'accent sur l'exploitation de documents bruts.

Finalement, j'ai eu l'id√©e de m'appuyer sur les fiches d'information publi√©es sur le site de [La communaut√© de l'inclusion](https://communaute.inclusion.gouv.fr), pour **proposer un agent conversationnel expert dans le domaine de l'insertion socio-professionnelle**.

J'ai cherch√© √† obtenir un chatbot capable de r√©pondre √† des questions sur l'IAE, les PMSMP et d'aider les professionnels de l'inclusion dans leur accompagnement de demandeurs d'emploi.

![La communaut√© de l'inclusion](communaute_inclusion.png)


## Conditions op√©rationnelles et mat√©rielles

J'ai con√ßu, d√©velopp√© et ex√©cut√© le programme exclusivement en local sur ma machine.

> üíª Il s'agit d'un Macbook Pro M2 Max, 64Go RAM, 12 c≈ìurs, sans carte graphique / GPU.

Je me suis fix√© comme contrainte de n'utiliser **que des mod√®les open-weight et des logiciels / briques open-source**.

J'ai ainsi utilis√© [Ollama](https://ollama.com/) comme serveur de mod√®les IA ouverts (thinking, embedding, vision, etc.).


## Collecte de donn√©es

La communaut√© de l'inclusion est une plateforme communautaire en ligne d'espaces de discussion sur des sujets relatifs √† l'insertion socio-professionelle. On y trouve des "forums" et des "topics". On y trouve surtout des "fiches pratiques" officielles, √©dit√©es et maintenues par la Plateforme de l'inclusion (PDI), de tr√®s grande qualit√© (fond, forme, fra√Æcheur).

Le point de d√©part de mon projet IA a donc consist√© √† r√©cup√©rer ces fameuses fiches d'information. Malheureusement, la PDI ne diffuse pas ces informations en open content sur data.gouv.fr.

Je n'ai pas eu d'autre choix que de d√©velopper un petit programme (en Node.js / TypeScript) pour crawler/scrapper les donn√©es directement depuis le site web. Les sources sont disponibles sur GitHub : [jbuget/crawler](https://github.com/jbuget/meilisearch-crawler/blob/main/src/crawler.ts).

Je me suis content√© de r√©cup√©rer les fiches pratiques, pas les questions / r√©ponses, car beaucoup de questions ne sont pas r√©ellement pertinentes ou directement corr√©l√©es au domaine, contiennent des donn√©es personnelles, ne sont pas r√©pondues ou sont des redites ou pointeurs vers les fiches pratiques.

C'est ainsi que j'ai obtenu un fichier JSON avec les URLs des fiches pratiques et le contenu textuel (HTML ‚Üí texte) des pages (merci Cheerio.js). Soit un corpus, en fran√ßais, de pr√®s d'un millier de documents.

```JSON
// data/topics.json

[
  {
    "id": "12513c26d6e647fc537447aad16bd9337988933bd44f0517cd3c0767b3f9bde0",
    "url": "https://communaute.inclusion.gouv.fr/forum/fiche-pratique-cellule-alerte-inclusion-163/",
    "title": "Fiche pratique Cellule-alerte-inclusion",
    "subtitle": "Vous √™tes intervenant social ou b√©n√©vole et vous estimez qu‚Äôune personne que vous suivez devrait b√©n√©ficier du dispositif de plafonnement des frais bancaires ou de l‚Äôoffre sp√©cifique client√®le fragile ? Vous accompagnez une personne d√©pourvue de compte bancaire ? Vous pouvez saisir la cellule alerte inclusion.",
    "content": "- lien vers la page de pr√©sentation sur banque-france.fr\n\n- lien vers le flyer de pr√©sentation"
  },
  {
    "id": "aa946e302c55270dffd388b3f289a443727651ad69df3c47d9adc3bcbddd9f09",
    "url": "https://communaute.inclusion.gouv.fr/forum/m%C3%A9mo-de-vie-prot%C3%A9ger-vos-documents-et-vos-t%C3%A9moignages-162/",
    "title": "M√©mo de vie - Prot√©ger vos documents et vos t√©moignages",
    "subtitle": "La plateforme M√©mo de Vie est destin√©e √† toute personne victime de violences sans distinction de genre, ni de sexe et en questionnement sur des violences subis.",
    "content": "lien vers memo-de-vie.org"
  },
  {
    "id": "e7ff95de2617b05b7e1ed0fbe059d5436029e8c4a964b782e5a8ef2e5deb6939",
    "url": "https://communaute.inclusion.gouv.fr/forum/outils-de-pr%C3%A9vention-prostitution-jeunes-161/",
    "title": "Outils de Pr√©vention Prostitution Jeunes",
    "subtitle": "Con√ßu par l‚ÄôAmicale du Nid. Ses objectifs : sensibiliser les jeunes √† la question de la prostitution, pr√©venir les risques prostitutionnels, informer sur les droits et le fait qu‚Äôun accompagnement est possible pour sortir de la prostitution.",
    "content": "lien vers la m√©diath√®que jenesuispasavendre.org"
  },
  ...
]
```


## Indexation vectorielle

Une fois la collecte des donn√©es r√©alis√©es, il a fallu les d√©verser de fa√ßon appropri√©e "quelque part" (spoiler alert : dans une base de donn√©es).

### 1. Choix du syst√®me de gestion de base de donn√©es (SGBD)

Ces derniers temps, j'ai beaucoup jou√© avec **Typesense** ou **Meilisearch**. Ces deux solutions sont des moteurs de recherche (textuelle) int√©grant de la recherche vectorielle pour des usages IA. Il existe des bases de donn√©es sp√©cialis√©es dans la recherche vectorielle comme **ChromaDB** (open-source, self-hostable ou SaaS), **Pinecone** (SaaS), **Qdrant** (open-source, self-hostable ou SaaS), **Weaviate** (open-source, self-hostable ou SaaS) ou **Milvus** (open-source, self-hostable ou SaaS).

Dans la mesure o√π j'ai relativement peu de documents et que mon serveur (c-√†-d ma machine) ne dispose pas de capacit√©s GPU, j'ai pr√©f√©r√© opter pour la solution **PostgreSQL + extension `pgvector`**, a priori adapt√©e pour des volumes et besoins de taille moyenne.

### 2. Cr√©ation de la base + table

√Ä l'initialisation de la base (script init.sql), j'ai commenc√© par activer l'extension pgvector.

Puis, j'ai cr√©√© la table "topics" avec une colonne "embedding".
Dans la mesure o√π j'ex√©cute le syst√®me sur mon poste, j'ai opt√© pour une configuration moyenne √† 768 dimensions.
Il semble pertinent de cibler 1536 dimensions pour du requ√™tage plus fin.

> ‚ö†Ô∏è Il faut bien faire attention √† retenir ce dimensionnement (768) lors du requ√™tage de donn√©es c√¥t√© back-end.

Enfin, j'ai d√©clar√© un index `idx_topics_embedding` pour optimiser les requ√™tes.

```sql
-- data/init.sql

CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE topics (
    id bigserial PRIMARY KEY,  
    embedding VECTOR(768), 
    title TEXT,
    subtitle TEXT,
    content TEXT,
    url TEXT UNIQUE
);

CREATE INDEX idx_topics_embedding ON topics USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);
```

### 3. Indexation des documents

L'indexation d'un document dans la base de donn√©es se passe comme suit :

* on ex√©cute un calcul de *vectorisation* (via un LLM) du contenu textuel du document
* on ins√®re dans la table topics une nouvelle entr√©e avec les champs structur√©s `url`, `title`, `subtitle` ainsi que le champs vectoris√© `embedding` 
* le requ√™tage de documents se fait alors en comparant le vecteur du prompt (obtenu via le m√™me mod√®le, √† la vol√©e) et les diff√©rents vecteurs stock√©s en base

C'est ici que j'ai rencontr√© des soucis de pertinence des r√©sultats et qu'il m'a fallu consid√©rer deux approches / types de mod√®les de vectorisation.


#### 3.1) Vectorisation des topics

Dans un premier temps, j'ai fait appel √† l'API de Ollama [`/api/embeddings`](https://docs.ollama.com/api#generate-embedding). ‚Äì dont le mod√®le sous-jacent par d√©faut est [`nomic-embed-text:v1.5`](https://ollama.com/library/) ‚Äì pour obtenir des vecteurs (a.k.a. "embeddings") de chaque document (rappel : des *topics*). √áa a sembl√© fonctionn√© techniquement, mais √† l'usage (requ√™tage), je trouvais les r√©sultats insatisfaisants. 

Certaines questions invoquaient les bons documents, mais beaucoup d'autres, notamment celles tournant autour d'acronymes ("PMSMP", "IAE", "CIP"), ne remontaient aucun documents correspondants. Les r√©ponses formul√©es par la partie RAG (cf. ci-dessous) n'√©taient donc pas tr√®s pertinentes ni exploitables pour de vrais utilisateurs. "Shit in, shit out".

J'ai tent√© avec plusieurs dimensionnements (1536) et mod√®les d'embedding propos√©s par Ollama (comme `mxbai-embed-large` (mars 2024)), mais aucun ne me donnait satisfaction. J'ai alors d√©cid√© de g√©n√©rer moi-m√™me les vecteurs de chaque topic.

> üí° En r√©digeant cet article, je d√©couvre que Ollama propose maintenant un mod√®le d'embedding plus puissant (et un peu plus lourd) : [bge-m3](https://ollama.com/library/bge-m3). Peut-√™tre aurait-il fait l'affaire.

Pour cela, j'ai utilis√© la biblioth√®que Sentence Transformers (a.k.a. [SBERT](https://www.sbert.net/)), avec l'√©volution du mod√®le standard de Ollama : [`nomic-embed-text-v2-moe`](todo), lequel impl√©mente le concept de [Mixtures of Experts](https://huggingface.co/blog/moe).

```python
# data/load_topics_with_embeddings.py

model = SentenceTransformer(
            model_name="nomic-ai/nomic-embed-text-v2-moe",
            device="cpu",
            trust_remote_code="true",
        )

text = [
    part.strip()
    for part in (
        topic.get("title", ""),
        topic.get("subtitle", ""),
        topic.get("content", ""),
    )
    if isinstance(part, str) and part.strip()
]

vector = model.encode(
            text,
            convert_to_numpy=True,
            normalize_embeddings=False,
            show_progress_bar=False,
        ).tolist()

# ...

return [float(value) for value in vector]
```

> üí° Lors du tout premier appel √† Sentence Transformers, celui-ci doit t√©l√©charger le mod√®le indiqu√© dans le client, ce qui peut prendre quelques minutes.

C'est ainsi que pour chaque topic, j'ai obtenu un vecteur propre calcul√© depuis les propri√©t√©s concat√©n√©es `title` + `subtitle` et `content` :

```python
# Input
title : "Fiche pratique Cellule-alerte-inclusion"
subtitle : "Vous √™tes intervenant social ou b√©n√©vole et vous estimez qu‚Äôune personne que vous suivez devrait b√©n√©ficier...""
content : "- lien vers la page de pr√©sentation sur banque-france.fr\n\n- lien vers le flyer de pr√©sentation..."

# Output
embedding : [-0.0030780921,-0.03730278,0.0012224227,0.001161514,-0.008647267,0.0004938656,-0.036171958,0.02440677,... (x760)]
```

#### 3.2) Insertion des documents en base

Une fois les vecteurs calcul√©s, il ne restait plus qu'√† enregistrer les donn√©es en base, avec un simple `INSERT INTO ... VALUES` :

```python
# data/load_topics_with_embeddings.py

def reset_and_insert_topics(
    conn: psycopg.Connection, payload: list[tuple[object, object, object, object, object]]
) -> int:
    with conn.cursor() as cur:
        cur.execute("TRUNCATE TABLE topics RESTART IDENTITY CASCADE")
        if payload:
            cur.executemany(
                """
                INSERT INTO topics (title, subtitle, content, url, embedding)
                VALUES (%s, %s, %s, %s, %s)
                """,
                payload,
            )

    return len(payload)
```

Nous obtenons alors ce type d'objets en base : 

```shell
chatbot=> select id, url, title, embedding from topics where url='https://communaute.inclusion.gouv.fr/forum/fiche-pratique-cellule-alerte-inclusion-163/';

-[ RECORD 1 ]-
id          | 47
url         | https://communaute.inclusion.gouv.fr/forum/fiche-pratique-cellule-alerte-inclusion-163/
title       | Fiche pratique Cellule-alerte-inclusion
subtitle    | Vous √™tes intervenant social ou b√©n√©vole et vous estimez qu‚Äôune personne que vous suivez devrait b√©n√©ficier...
embedding   | [-0.0030780921,-0.03730278,0.0012224227,0.001161514,-0.008647267,0.0004938656,-0.036171958,0.02440677,... (x760)]
```

√Ä ce stade, tout √©tait pr√™t pour enfin passer aux choses s√©rieuses : d√©velopper une API exploitant la recherche vectorielle et formulant une r√©ponse RAG sur la base et limit√©e aux documents r√©sultants.

## Prompting et g√©n√©ration de r√©ponse

## IHM

## Conclusion

