---
date: 2025-10-15T09:02:00+02:00
draft: false
toc: true
params:
  author: JÃ©rÃ©my Buget
title: DÃ©velopper un agent IA custom
categories:
  - IA
  - R&D
  - DÃ©veloppement
tags:
  - Chatbot
  - LLM
  - RAG
  - Ollama
  - IA open-source
  - FastAPI
  - Python
  - pgvector
description: >
  Retour dâ€™expÃ©rience sur la conception dâ€™un chatbot mÃ©tier basÃ© sur un corpus privÃ©, 
  en combinant vectorisation, recherche sÃ©mantique et gÃ©nÃ©ration de rÃ©ponses avec des modÃ¨les open-weight (Ollama, Sentence Transformers, FastAPI, PostgreSQL).
cover:
  image: "chatbot_inclusion_pmsmp-definition.png"
  alt: "Capture dâ€™Ã©cran du chatbot IA dÃ©diÃ© Ã  lâ€™inclusion socio-professionnelle"
  caption: "Chatbot IA â€“ DÃ©monstration dâ€™un agent RAG sur corpus mÃ©tier"
  relative: true
meta:
  ogType: "article"
  ogImage: "chatbot_inclusion_pmsmp-definition.png"
  twitterCard: "summary_large_image"
  twitterCreator: "@jbuget"
  keywords: ["chatbot", "IA", "open-weight", "RAG", "Ollama", "FastAPI", "pgvector", "Sentence Transformers"]

---

## Introduction

Comme tout le monde, j'ai intÃ©grÃ© et continue d'incorporer de plus en plus de pratiques, raisonnements et outils IA dans mon quotidien, pro ou perso : **agents conversationnels** (ChatGPT, Gemini) pour comprendre des choses et gÃ©nÃ©rer / amÃ©liorer du contenu ; **compagnons techniques** (Codex, Claude Code, Zed) pour m'aider Ã  produire et gÃ©rer du code ; **algorithmes et modÃ¨les** (LLM, OCR, STT/TTS) pour rÃ©aliser des tÃ¢ches relativement compliquÃ©es ou rÃ©barbatives comme extraire, modifier ou transformer des informations sous toutes les formes.

Savoir utiliser cette technologie et ce qu'elle implique me paraÃ®t dÃ©sormais une compÃ©tence professionnelle essentielle, quel que soit le mÃ©tier en question, relatif Ã  l'informatique ou non. En tant que dÃ©veloppeur, j'ai cherchÃ© Ã  aller un cran plus loin et explorÃ© comment concevoir et produire un outil exploitant l'IA *dans* et *pour* un domaine fonctionnel particulier.

C'est ainsi que j'ai rÃ©alisÃ© un petit chatbot capable d'interroger un corpus prÃ©dÃ©fini de documents spÃ©cifiques pour Ã©laborer des rÃ©ponses sourcÃ©es, dans la limite du champs dÃ©fini.

## TL;DR

- J'ai dÃ©veloppÃ© un chatbot IA spÃ©cialisÃ© dans le domaine de l'inclusion socio-professionnelle
- Cet agent se base sur un corpus de documents issus du site [La communautÃ© de l'inclusion](https://communaute.inclusion.gouv.fr/)
- J'ai dÃ©marrÃ© un serveur Ollama avec le modÃ¨le open-weight de OpenAI : `gpt-oss:20b`
- J'ai codÃ© **un script de crawling** (en Node.js, avec Cheerio.js) pour rÃ©cupÃ©rer des fiches d'information
- J'ai installÃ© **une base de donnÃ©es PostgreSQL** et activÃ© l'extension **pgvector**
- J'ai dÃ©clarÃ© une base avec une table disposant d'une colonne de type `embedding VECTOR(768)`
- J'ai vectorisÃ© ces fiches grÃ¢ce Ã  la lib python **Sentence Transformers** (768 dimensions) et au modÃ¨le `nomic-ai/nomic-embed-text-v2-moe`
- J'ai indexÃ© les embeddings obtenus dans la base PG
- J'ai dÃ©veloppÃ© **une API (Python/FastAPI)** pour mobiliser les diffÃ©rentes briques IA (query + RAG) sollicitÃ©es
- Lorsqu'un utilisateur interroge le chatbot, son prompt est Ã  son tour vectorisÃ© pour effectuer une recherche par comparaison de vecteurs. On obtient ainsi un ensemble de "documents sources" correspondants (avec un pourcentage de pertinence)
- Je passe alors ces documents comme Ã©lÃ©ments exclusifs de "contexte" dans l'instruction que soumets au **LLM responsable de la gÃ©nÃ©ration de la rÃ©ponse** (en interrogeant `/api/chat` de l'API exposÃ©e par Ollama)
- Je passe aussi au LLM les autres (prÃ©cÃ©dents) messages de la conversation en cours
- J'ai dÃ©veloppÃ© **une webapp chatbot-like** toute simple pour poser des questions et afficher les rÃ©ponses
- Le code source est disponible sur GitHub : [jbuget/ia-custom-chatbot](https://github.com/jbuget/ia-custom-chatbot)

![Diagramme de flux de l'application](ia_custom_chatbot.jpg)

## Objectifs

Mon objectif, Ã  travers ce POC (proof of concept), Ã©tait de **comprendre comment exploiter l'IA d'un point de vue vÃ©ritablement mÃ©tier, plus seulement comme outil de production ou aide Ã  la crÃ©ation**. Je souhaitais aussi dÃ©couvrir les solutions envisageables, Ã©valuer leur potentiel et me confronter Ã  leurs limites / contraintes.

ConcrÃ¨tement, je voulais concevoir une API / webapp capable de d'interagir (Ã©couter et rÃ©pondre) en langage naturel, avec des utilisateurs spÃ©cifiques d'un domaine, pour rÃ©pondre le plus prÃ©cisÃ©ment Ã  des questions mÃ©tier, en s'appuyant et en citant des sources que j'aurais au prÃ©alable collectÃ©es et indexÃ©es.

![Capture d'Ã©cran du chatbot (question + rÃ©ponse)](chatbot_inclusion_pmsmp-definition.png)

## Cas d'usage

J'ai pas mal cherchÃ© sur quel champs fonctionnel baser le projet.

Ayant beaucoup Å“uvrÃ© dans le service public ces derniÃ¨res annÃ©es, j'ai d'abord creusÃ© du cÃ´tÃ© de [data.gouv.fr](https://data.gouv.fr). S'il y a Ã©normÃ©ment de donnÃ©es rendues accessibles par les administrations, les collectivitÃ©s et leurs partenaires, la plupart des datasets sont des donnÃ©es fortement structurÃ©es, en colonnes. Pour mon cas d'usage, je souhaitais de la donnÃ©es moins structurÃ©es, Ã  base de contenu et textes riches.

Je me suis tournÃ© vers les sites et plateformes de datasets, comme [Kaggle](https://www.kaggle.com/datasets). Mais lÃ  encore, c'Ã©tait beaucoup de donnÃ©es structurÃ©es, de qualitÃ© Ã  gÃ©omÃ©trie vraiment variable.

Je pense qu'on retrouve ce type de donnÃ©es prÃ©cieuses en entreprise, et qu'il faut pouvoir les intÃ©grer et en tenir compte lors de requÃªtes utilisateurs, mais dans le cadre de mon projet d'IA, je voulais vraiment mettre l'accent sur l'exploitation de documents bruts.

Finalement, j'ai eu l'idÃ©e de m'appuyer sur les fiches d'information publiÃ©es sur le site de [La communautÃ© de l'inclusion](https://communaute.inclusion.gouv.fr), pour **proposer un agent conversationnel expert dans le domaine de l'insertion socio-professionnelle**.

J'ai cherchÃ© Ã  obtenir un chatbot capable de rÃ©pondre Ã  des questions sur l'IAE, les PMSMP et d'aider les professionnels de l'inclusion dans leur accompagnement de demandeurs d'emploi.

![La communautÃ© de l'inclusion](communaute_inclusion.png)


## Conditions opÃ©rationnelles et matÃ©rielles

J'ai conÃ§u, dÃ©veloppÃ© et exÃ©cutÃ© le programme exclusivement en local sur ma machine.

> ğŸ’» Il s'agit d'un Macbook Pro M2 Max, 64Go RAM, 12 cÅ“urs, sans carte graphique / GPU.

Je me suis fixÃ© comme contrainte de n'utiliser **que des modÃ¨les open-weight et des logiciels / briques open-source**.

J'ai ainsi utilisÃ© [Ollama](https://ollama.com/) comme serveur de modÃ¨les IA ouverts (thinking, embedding, vision, etc.).


## Collecte de donnÃ©es

La communautÃ© de l'inclusion est une plateforme communautaire en ligne d'espaces de discussion sur des sujets relatifs Ã  l'insertion socio-professionelle. On y trouve des "forums" et des "topics". On y trouve surtout des "fiches pratiques" officielles, Ã©ditÃ©es et maintenues par la Plateforme de l'inclusion (PDI), de trÃ¨s grande qualitÃ© (fond, forme, fraÃ®cheur).

Le point de dÃ©part de mon projet IA a donc consistÃ© Ã  rÃ©cupÃ©rer ces fameuses fiches d'information. Malheureusement, la PDI ne diffuse pas ces informations en open content sur data.gouv.fr.

Je n'ai pas eu d'autre choix que de dÃ©velopper un petit programme (en Node.js / TypeScript) pour crawler/scrapper les donnÃ©es directement depuis le site web. Les sources sont disponibles sur GitHub : [jbuget/crawler](https://github.com/jbuget/meilisearch-crawler/blob/main/src/crawler.ts).

Je me suis contentÃ© de rÃ©cupÃ©rer les fiches pratiques, pas les questions / rÃ©ponses, car beaucoup de questions ne sont pas rÃ©ellement pertinentes ou directement corrÃ©lÃ©es au domaine, contiennent des donnÃ©es personnelles, ne sont pas rÃ©pondues ou sont des redites ou pointeurs vers les fiches pratiques.

C'est ainsi que j'ai obtenu un fichier JSON avec les URLs des fiches pratiques et le contenu textuel (HTML â†’ texte) des pages (merci Cheerio.js). Soit un corpus, en franÃ§ais, de prÃ¨s d'un millier de documents.

```JSON
// data/topics.json

[
  {
    "id": "12513c26d6e647fc537447aad16bd9337988933bd44f0517cd3c0767b3f9bde0",
    "url": "https://communaute.inclusion.gouv.fr/forum/fiche-pratique-cellule-alerte-inclusion-163/",
    "title": "Fiche pratique Cellule-alerte-inclusion",
    "subtitle": "Vous Ãªtes intervenant social ou bÃ©nÃ©vole et vous estimez quâ€™une personne que vous suivez devrait bÃ©nÃ©ficier du dispositif de plafonnement des frais bancaires ou de lâ€™offre spÃ©cifique clientÃ¨le fragile ? Vous accompagnez une personne dÃ©pourvue de compte bancaire ? Vous pouvez saisir la cellule alerte inclusion.",
    "content": "- lien vers la page de prÃ©sentation sur banque-france.fr\n\n- lien vers le flyer de prÃ©sentation"
  },
  {
    "id": "aa946e302c55270dffd388b3f289a443727651ad69df3c47d9adc3bcbddd9f09",
    "url": "https://communaute.inclusion.gouv.fr/forum/m%C3%A9mo-de-vie-prot%C3%A9ger-vos-documents-et-vos-t%C3%A9moignages-162/",
    "title": "MÃ©mo de vie - ProtÃ©ger vos documents et vos tÃ©moignages",
    "subtitle": "La plateforme MÃ©mo de Vie est destinÃ©e Ã  toute personne victime de violences sans distinction de genre, ni de sexe et en questionnement sur des violences subis.",
    "content": "lien vers memo-de-vie.org"
  },
  {
    "id": "e7ff95de2617b05b7e1ed0fbe059d5436029e8c4a964b782e5a8ef2e5deb6939",
    "url": "https://communaute.inclusion.gouv.fr/forum/outils-de-pr%C3%A9vention-prostitution-jeunes-161/",
    "title": "Outils de PrÃ©vention Prostitution Jeunes",
    "subtitle": "ConÃ§u par lâ€™Amicale du Nid. Ses objectifs : sensibiliser les jeunes Ã  la question de la prostitution, prÃ©venir les risques prostitutionnels, informer sur les droits et le fait quâ€™un accompagnement est possible pour sortir de la prostitution.",
    "content": "lien vers la mÃ©diathÃ¨que jenesuispasavendre.org"
  },
  ...
]
```


## Indexation vectorielle

Une fois la collecte des donnÃ©es rÃ©alisÃ©es, il a fallu les dÃ©verser de faÃ§on appropriÃ©e "quelque part" (spoiler alert : dans une base de donnÃ©es).

### 1. Choix du systÃ¨me de gestion de base de donnÃ©es (SGBD)

Ces derniers temps, j'ai beaucoup jouÃ© avec **Typesense** ou **Meilisearch**. Ces deux solutions sont des moteurs de recherche (textuelle) intÃ©grant de la recherche vectorielle pour des usages IA. Il existe des bases de donnÃ©es spÃ©cialisÃ©es dans la recherche vectorielle comme **ChromaDB** (open-source, self-hostable ou SaaS), **Pinecone** (SaaS), **Qdrant** (open-source, self-hostable ou SaaS), **Weaviate** (open-source, self-hostable ou SaaS) ou **Milvus** (open-source, self-hostable ou SaaS).

Dans la mesure oÃ¹ j'ai relativement peu de documents et que mon serveur (c-Ã -d ma machine) ne dispose pas de capacitÃ©s GPU, j'ai prÃ©fÃ©rÃ© opter pour la solution **PostgreSQL + extension `pgvector`**, a priori adaptÃ©e pour des volumes et besoins de taille moyenne.

### 2. CrÃ©ation de la base + table

Ã€ l'initialisation de la base (script init.sql), j'ai commencÃ© par activer l'extension pgvector.

Puis, j'ai crÃ©Ã© la table "topics" avec une colonne "embedding".
Dans la mesure oÃ¹ j'exÃ©cute le systÃ¨me sur mon poste, j'ai optÃ© pour une configuration moyenne Ã  768 dimensions.
Il semble pertinent de cibler 1536 dimensions pour du requÃªtage plus fin.

> âš ï¸ Il faut bien faire attention Ã  retenir ce dimensionnement (768) lors du requÃªtage de donnÃ©es cÃ´tÃ© back-end.

Enfin, jâ€™ai dÃ©clarÃ© un index IVFFLAT `idx_topics_embedding` pour accÃ©lÃ©rer la recherche vectorielle.

```sql {hl_lines=[14]}
-- data/init.sql

CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE topics (
    id bigserial PRIMARY KEY,  
    embedding VECTOR(768), 
    title TEXT,
    subtitle TEXT,
    content TEXT,
    url TEXT UNIQUE
);

CREATE INDEX idx_topics_embedding ON topics USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);
```

Lâ€™extension `pgvector` supporte plusieurs types dâ€™index pour la recherche de similaritÃ©, dont les deux plus courants sont `ivfflat` (Inverted File Flat) et `hnsw` (Hierarchical Navigable Small World Graph).

* IVFFLAT est plus adaptÃ© pour des volumes de donnÃ©es moyens (quelques milliers Ã  centaines de milliers dâ€™Ã©lÃ©ments) ou quand on dispose de ressources plus limitÃ©es (mÃ©moire, espace disque) ; Ã  noter que le temps d'indexation est plus rÃ©duit ;
* HNSW est plus adaptÃ© pour les larges jeux de donnÃ©es, pour des donnÃ©es qui changent rÃ©guliÃ¨rement, pour des recherches plus rapides et lorsque l'on dispose de ressources supÃ©rieures (ex : GPU)

> ğŸ’¡ Si on envisage de manipuler des millions de vecteurs ou de faire Ã©voluer rÃ©guliÃ¨rement la base, HNSW serait un meilleur choix. > Pour des projets exploratoires ou des dÃ©monstrations, comme c'est mon cas ici, IVFFLAT reste la solution la plus pragmatique et lÃ©gÃ¨re.

Pour aller plus loin, je vous invite Ã  lire ce billet : "[PGVector: HNSW vs IVFFlat â€” A Comprehensive Study](https://medium.com/@bavalpreetsinghh/pgvector-hnsw-vs-ivfflat-a-comprehensive-study-21ce0aaab931)".

### 3. Indexation des documents

L'indexation d'un document dans la base de donnÃ©es se passe comme suit :

* on exÃ©cute un calcul de *vectorisation* (via un LLM) du contenu textuel du document
* on insÃ¨re dans la table topics une nouvelle entrÃ©e avec les champs structurÃ©s `url`, `title`, `subtitle` ainsi que le champs vectorisÃ© `embedding` 
* le requÃªtage de documents se fait alors en comparant le vecteur du prompt (obtenu via le mÃªme modÃ¨le, Ã  la volÃ©e) et les diffÃ©rents vecteurs stockÃ©s en base

C'est ici que j'ai rencontrÃ© des soucis de pertinence des rÃ©sultats et qu'il m'a fallu considÃ©rer deux approches / types de modÃ¨les de vectorisation.


#### 3.1) Vectorisation des topics

Dans un premier temps, j'ai fait appel Ã  l'API de Ollama [`/api/embeddings`](https://docs.ollama.com/api#generate-embedding). â€“ dont le modÃ¨le sous-jacent par dÃ©faut est [`nomic-embed-text:v1.5`](https://ollama.com/library/) â€“ pour obtenir des vecteurs (a.k.a. "embeddings") de chaque document (rappel : des *topics*). Ã‡a a semblÃ© fonctionnÃ© techniquement, mais Ã  l'usage (requÃªtage), je trouvais les rÃ©sultats insatisfaisants. 

Certaines questions invoquaient les bons documents, mais beaucoup d'autres, notamment celles tournant autour d'acronymes ("PMSMP", "IAE", "CIP"), ne remontaient aucun documents correspondants. Les rÃ©ponses formulÃ©es par la partie RAG (cf. ci-dessous) n'Ã©taient donc pas trÃ¨s pertinentes ni exploitables pour de vrais utilisateurs. "Shit in, shit out".

J'ai tentÃ© avec plusieurs dimensionnements (1536) et modÃ¨les d'embedding proposÃ©s par Ollama (comme `mxbai-embed-large` (mars 2024)), mais aucun ne me donnait satisfaction. J'ai alors dÃ©cidÃ© de gÃ©nÃ©rer moi-mÃªme les vecteurs de chaque topic.

> ğŸ’¡ En rÃ©digeant cet article, je dÃ©couvre que Ollama propose maintenant un modÃ¨le d'embedding plus puissant (et un peu plus lourd) : [bge-m3](https://ollama.com/library/bge-m3). Peut-Ãªtre aurait-il fait l'affaire.

Pour cela, j'ai utilisÃ© la bibliothÃ¨que Sentence Transformers (a.k.a. [SBERT](https://www.sbert.net/)), avec l'Ã©volution du modÃ¨le standard de Ollama : [`nomic-embed-text-v2-moe`](todo), lequel implÃ©mente le concept de [Mixtures of Experts](https://huggingface.co/blog/moe).

```python
# data/load_topics_with_embeddings.py

model = SentenceTransformer(
            model_name="nomic-ai/nomic-embed-text-v2-moe",
            device="cpu",
            trust_remote_code="true",
        )

text = [
    part.strip()
    for part in (
        topic.get("title", ""),
        topic.get("subtitle", ""),
        topic.get("content", ""),
    )
    if isinstance(part, str) and part.strip()
]

vector = model.encode(
            text,
            convert_to_numpy=True,
            normalize_embeddings=False,
            show_progress_bar=False,
        ).tolist()

# ...

return [float(value) for value in vector]
```

> ğŸ’¡ Lors du tout premier appel Ã  Sentence Transformers, celui-ci doit tÃ©lÃ©charger le modÃ¨le indiquÃ© dans le client, ce qui peut prendre quelques minutes.

C'est ainsi que pour chaque topic, j'ai obtenu un vecteur propre calculÃ© depuis les propriÃ©tÃ©s concatÃ©nÃ©es `title` + `subtitle` et `content` :

```python
# Input
title : "Fiche pratique Cellule-alerte-inclusion"
subtitle : "Vous Ãªtes intervenant social ou bÃ©nÃ©vole et vous estimez quâ€™une personne que vous suivez devrait bÃ©nÃ©ficier...""
content : "- lien vers la page de prÃ©sentation sur banque-france.fr\n\n- lien vers le flyer de prÃ©sentation..."

# Output
embedding : [-0.0030780921,-0.03730278,0.0012224227,0.001161514,-0.008647267,0.0004938656,-0.036171958,0.02440677,... (x760)]
```

#### 3.2) Insertion des documents en base

Une fois les vecteurs calculÃ©s, il ne restait plus qu'Ã  enregistrer les donnÃ©es en base, avec un simple `INSERT INTO ... VALUES` :

```python
# data/load_topics_with_embeddings.py

def reset_and_insert_topics(
    conn: psycopg.Connection, payload: list[tuple[object, object, object, object, object]]
) -> int:
    with conn.cursor() as cur:
        cur.execute("TRUNCATE TABLE topics RESTART IDENTITY CASCADE")
        if payload:
            cur.executemany(
                """
                INSERT INTO topics (title, subtitle, content, url, embedding)
                VALUES (%s, %s, %s, %s, %s)
                """,
                payload,
            )

    return len(payload)
```

Nous obtenons alors ce type d'objets en base : 

```shell
chatbot=> select id, url, title, embedding from topics where url='https://communaute.inclusion.gouv.fr/forum/fiche-pratique-cellule-alerte-inclusion-163/';

-[ RECORD 1 ]-
id          | 47
url         | https://communaute.inclusion.gouv.fr/forum/fiche-pratique-cellule-alerte-inclusion-163/
title       | Fiche pratique Cellule-alerte-inclusion
subtitle    | Vous Ãªtes intervenant social ou bÃ©nÃ©vole et vous estimez quâ€™une personne que vous suivez devrait bÃ©nÃ©ficier...
embedding   | [-0.0030780921,-0.03730278,0.0012224227,0.001161514,-0.008647267,0.0004938656,-0.036171958,0.02440677,... (x760)]
```

Tout Ã©tait prÃªt dÃ©sormais pour passer Ã  la suite : dÃ©velopper une API exploitant la recherche vectorielle et formulant une rÃ©ponse RAG sur la base et limitÃ©e aux documents rÃ©sultants.

## Conception de l'API

Ã€ ce stade, je possÃ©dais une base avec des donnÃ©es prÃªtes Ã  Ãªtre requÃªtÃ©es et passÃ©es en contexte d'instructions donnÃ©es Ã  un LLM, chargÃ© lui de gÃ©nÃ©rer une rÃ©ponse Ã  l'utilisateur, en langage naturel, sur la base des documents remontÃ©s.

J'ai commencÃ© par initialiser un projet Python / FastAPI, qui expose le endpoint `POST /api/ask`.

Son fonctionnement est le suivant :

1. on extrait les paramÃ¨tres de la requÃªtes HTTP, en particulier la question (phrase)
2. on vectorise la question, avec le mÃªme algo / modÃ¨le utilisÃ© pour indexer les topics
3. on effectue une recherche des topics par comparaison vectorielle avec celui de la question
4. on gÃ©nÃ¨re une rÃ©ponse via un second LLM en lui passant en contexte les documents remontÃ©s
5. on formatte et renvoie un objet JSON dans la rÃ©ponse HTTP

Cette route prend en paramÃ¨tres :

- `question` : la question utilisateur Ã  interprÃ©ter
- `top_k` (optionnel) : le nombre maximum de documents Ã  citer.

```shell
# Request HTTP

curl --request POST \
  --url http://localhost:8000/api/v1/ask \
  --header 'Content-Type: application/json' \
  --data '{
	"question": "Qu'\''est-ce qu'\''une PMSMP ?",
	"top_k": "5"
}'
```

Elle retourne un objet JSON avec les champs :

- `answer` : la rÃ©ponse formulÃ©e par l'assistant
- `documents` : les documents citÃ©s pour appuyer la rÃ©ponse

```shell
# Response HTTP (extrait)

{
	"answer": "**Quâ€™estâ€‘ce quâ€™une PMSMPâ€¯?**\n\n- **PÃ©riode de Mise en Situation en Milieu Professionnel (PMSMP)**  \n  Il sâ€™agit dâ€™un dispositif dâ€™immersion professionnelle proposÃ© Ã  toute personne, quel que soit son statut ou son Ã¢ge, lorsquâ€™elle bÃ©nÃ©ficie dâ€™un accompagnement social ou professionnel personnalisÃ©â€¯:  \n  - personnes sans activitÃ© en parcours dâ€™insertion (recherche dâ€™emploi, jeunes accompagnÃ©s par la Mission locale, bÃ©nÃ©ficiaires du RSA, etc.)  \n  - personnes en activitÃ© engagÃ©es dans une dÃ©marche dâ€™insertion ou de rÃ©orientation professionnelle (salariÃ©s en rÃ©orientation, accompagnÃ©s par une SIAE, salariÃ©s en CUI, etc.).  \n  Cette pÃ©riode permet Ã  la personne de vivre une expÃ©rience concrÃ¨te dans un environnement de travail rÃ©el, afin dâ€™identifier ses forces, ses besoins de formation et dâ€™enrichir son projet dâ€™emploi.  \n  *(sourceâ€¯:â€¯[Doc1])*\n\n- **Cadre de lâ€™expÃ©rience accompagnÃ©e**  \n  Dans le modÃ¨le dâ€™accompagnement vers lâ€™emploi Â«â€¯expÃ©rience accompagnÃ©eâ€¯Â», la PMSMP constitue lâ€™un des outils clÃ©s. Elle est combinÃ©e aux Actions de Formation en Situation de Travail (AFEST) et Ã  la PrÃ©paration OpÃ©rationnelle Ã  lâ€™Emploi (POEI / POEC) pour offrir des immersions rÃ©elles et des occasions dâ€™apprentissage sur le terrain.  \n  *(sourceâ€¯:â€¯[Doc2])*\n\nEn rÃ©sumÃ©, une PMSMP est une pÃ©riode dâ€™immersion professionnelle destinÃ©e Ã  aider les personnes Ã©loignÃ©es de lâ€™emploi Ã  tester, acquÃ©rir et valoriser des compÃ©tences dans un cadre rÃ©el, dans le cadre dâ€™un accompagnement individualisÃ©.",
	"documents": [
		{
			"rank": 1,
			"topic_id": 68,
			"title": "La PMSPM : PÃ©riode de Mise en Situation en Milieu Professionnel",
			"url": "https://communaute.inclusion.gouv.fr/forum/la-pmspm-p%C3%A9riode-de-mise-en-situation-en-milieu-professionnel-124/",
			"excerpt": "- Les PMSMP sont ouvertes sur prescription Ã  toute personne quel que soit son statut ou son Ã¢ge, dÃ¨s lors qu'elle fait l'objet d'unâ€¦",
			"similarity": 0.6959141391237493
		},
		{
			"rank": 2,
			"topic_id": 87,
			"title": "ğŸ¤ğŸ‘©L'expÃ©rience accompagnÃ©e des publics Ã©loignÃ©s de l'emploi",
			"url": "https://communaute.inclusion.gouv.fr/forum/lexp%C3%A9rience-accompagn%C3%A9e-des-publics-%C3%A9loign%C3%A9s-de-lemploi-190/",
			"excerpt": "L'expÃ©rience accompagnÃ©e est une mÃ©thode d'accompagnement vers l'emploi qui propose un changement de paradigme par rapport aux approchesâ€¦",
			"similarity": 0.5818639535972785
		},
		{ ... },
		{ ... },
		{ ... }
	]
}
```

## Recherche sÃ©mantique par comparaison vectorielle

> ğŸ’¡ Par souci de lisibilitÃ©, j'ai volontairement rÃ©duit et simplifiÃ© le code ci-dessous. Pour rappel, tout le code est disponible [sur mon GitHub](https://github.com/jbuget/ia-custom-chatbot/).

En premier lieu, on calcule "l'embedding" ou "vecteur" de la question. 

Pour que la comparaison sÃ©mantique entre la question et les documents indexÃ©s en base de donnÃ©es puisse se faire, **il faut s'assurer d'utiliser le mÃªme modÃ¨le de calcul d'embedding**, ici `nomic-ai/nomic-embed-text-v2`.

On obtient un tableau de 768 valeurs flottantes, soit le nombre de dimensions dÃ©finies pour le champs embeddings de la table topics.

```python
# Calcul du vecteur de la question

model = await asyncio.to_thread(
    SentenceTransformer,
    "nomic-ai/nomic-embed-text-v2",
    "cpu",
    trust_remote_code=True,
)

vector = await asyncio.to_thread(
    model.encode,
    text,
    show_progress_bar=False,
    convert_to_numpy=True,
    normalize_embeddings=False,
)

values = vector.tolist()

embedding = [float(value) for value in values]

return embedding
```

Une fois cet embedding obtenu, il suffit de prÃ©parer et exÃ©cuter une requÃªte SQL qui trie les documents correspondant Ã  la question par rapport Ã  leur "proximitÃ©" ou "similaritÃ©".

**La mÃ©trique de similaritÃ© utilisÃ©e ici pour effectuer la comparaison est la `distance cosine`.**

> Il existe plusieurs mÃ©triques de similaritÃ© utilisÃ©es pour comparer des vecteurs selon le type dâ€™application :
> - **la similaritÃ© cosinus** (utilisÃ©e ici, via l'opÃ©rateur `<=>`) mesure lâ€™angle entre deux vecteurs et est couramment utilisÃ©e pour la recherche sÃ©mantique sur du texte, car elle Ã©value la proximitÃ© de sens indÃ©pendamment de la longueur des vecteurs.
> - **la distance euclidienne (L2)** (via l'opÃ©rateur `<->`) calcule la distance gÃ©omÃ©trique directe entre les points et convient mieux Ã  des cas oÃ¹ la position ou la magnitude ont du sens, comme la recherche gÃ©ospatiale ou les donnÃ©es physiques.
> - **le produit scalaire (Inner Product)** tient compte Ã  la fois de lâ€™orientation et de la taille des vecteurs, ce qui le rend particuliÃ¨rement adaptÃ© aux systÃ¨mes de recommandation, oÃ¹ lâ€™intensitÃ© dâ€™une prÃ©fÃ©rence ou dâ€™une relation est importante.

```python
# RequÃªtage de la BDD

pool = get_pool()
    
vector = f"[{", ".join(f"{value:.10f}" for value in embedding)}]"

async with pool.connection() as conn:
    async with conn.cursor(row_factory=dict_row) as cursor:
        await cursor.execute(
            """
            SELECT
                id,
                title,
                subtitle,
                content,
                url,
                1 / (1 + (embedding <=> %s)) AS similarity
            FROM topics
            WHERE embedding IS NOT NULL
            ORDER BY embedding <=> %s
            LIMIT %s
            """,
            (vector, vector, limit),
        )
        rows = await cursor.fetchall()

        if rows:
            return rows

return rows
```

C'est ainsi qu'on se retrouve avec un tableau de `top_k` documents correspondant sÃ©mantiquement Ã  la `question` posÃ©e.

## GÃ©nÃ©ration de la rÃ©ponse

Maintenant que nous disposons des topics susceptibles de fournir une rÃ©ponse Ã  notre question, nous pouvons les utiliser pour gÃ©nÃ©rer une rÃ©ponse formelle en langage naturel.
Pour cela, nous utilisons un second modÃ¨le LLM, dit de raisonnement et gÃ©nÃ©ration de contenu textuel.

Pour l'expÃ©rience, j'ai testÃ© plusieurs LLMs open weight.
J'ai obtenu les meilleurs par rapport Ã  ma machine et au temps acceptable (60s) avec `gpt-oss:20b`.

### 1. Instructions systÃ¨me

Les *instructions systÃ¨me* dÃ©finissent le rÃ´le du modÃ¨le et le cadre gÃ©nÃ©ral dans lequel il doit opÃ©rer.
On peut les voir comme la personnalitÃ© â€œsqueletteâ€ du modÃ¨le â€” ce qui va orienter son comportement quel que soit le sujet.
Elles jouent le rÃ´le de "personnalitÃ©" et de garde-fou conceptuel du LLM : elles lui rappellent ce quâ€™il est censÃ© faire, comment il doit rÃ©pondre, et surtout ce quâ€™il ne doit pas faire.

Dans notre cas, le modÃ¨le est explicitement positionnÃ© comme un assistant expert du domaine de lâ€™insertion socio-professionnelle et des politiques publiques associÃ©es.
Ces consignes garantissent que le modÃ¨le reste centrÃ© sur le pÃ©rimÃ¨tre mÃ©tier et quâ€™il sâ€™exprime en franÃ§ais, de maniÃ¨re rigoureuse, structurÃ©e et sourcÃ©e.

Elles imposent Ã©galement plusieurs contraintes essentielles :

* Limiter la rÃ©ponse aux seules donnÃ©es issues du corpus interne (les fiches documentaires).
* Citer les identifiants des documents utilisÃ©s ([Doc1], [Doc2], etc.), afin dâ€™assurer la traÃ§abilitÃ© et la vÃ©rifiabilitÃ© des rÃ©ponses.
* Signaler explicitement les lacunes ou les incertitudes du corpus, au lieu de produire des approximations.

Lâ€™objectif de ce prompt systÃ¨me est donc de verrouiller le comportement du modÃ¨le pour quâ€™il reste un agent dâ€™analyse documentaire fiable, et non un gÃ©nÃ©rateur de texte gÃ©nÃ©raliste.

```python
# Prompt systÃ¨me

system_prompt = (
    "Vous Ãªtes un assistant expert spÃ©cialisÃ© dans le milieu de lâ€™insertion socioâ€‘professionnelle, Ã  lâ€™accompagnement des personnes Ã©loignÃ©es de lâ€™emploi, et aux dispositifs publics en France (ex. PMSMP, accompagnement, dispositif public, prestataires, droits, obligations).\n"
    "Vous devez :\n"
    "1. RÃ©pondre **en franÃ§ais**, de faÃ§on claire, factuelle, structurÃ©e (paragraphes, listes si utile).\n"
    "2. Ne mentionner dans votre rÃ©ponse que les informations **strictement issues des documents de la base** (les fiches scrappÃ©es).\n"
    "3. Chaque fois que vous citez une donnÃ©e / rÃ¨gle / information provenant dâ€™une fiche, indiquer explicitement son identifiant (ex. `[Doc12]`, `[Doc5]`).\n"
    "4. Si une question demande une information **non prÃ©sente dans les documents**, lâ€™indiquer clairement, de sorte que lâ€™utilisateur sache que la source nâ€™a pas fourni cette rÃ©ponse.\n"
    "5. Ne pas halluciner : ne pas inventer des dispositifs, articles ou chiffres non prÃ©sents dans vos documents, sauf si vous avez la certitude (et toujours en prÃ©cisant la source).\n"
    "6. Si la question porte sur une mise Ã  jour rÃ©cente (loi, jurisprudence) ou une zone dâ€™incertitude, vous pouvez signaler les limites, et recommander Ã  lâ€™utilisateur de vÃ©rifier les textes officiels ou sources actualisÃ©es."
    "\n\n"
    "MÃªme si aucune rÃ©ponse exacte nâ€™est disponible, propose des Ã©lÃ©ments proches ou des dÃ©marches pour trouver lâ€™information recherchÃ©e.\n"
    "\n\n"
    "**Objectif :** servir de â€œpoint de vÃ©ritÃ©â€ extrait des fiches de la â€œCommunautÃ© de lâ€™Inclusionâ€, et aider lâ€™utilisateur Ã  approfondir ses recherches via ces documents internes.\n" 
)
```

### 2. Instructions utilisateur

Les *instructions utilisateur* servent Ã  contextualiser la requÃªte ponctuelle adressÃ©e au modÃ¨le et Ã  guider la gÃ©nÃ©ration de la rÃ©ponse.
Elles traduisent la question de lâ€™utilisateur (`query`) et le contexte documentaire pertinent (`context`) issu de la phase de recherche sÃ©mantique.
Autrement dit, câ€™est Ã  ce stade que le modÃ¨le reÃ§oit les Ã©lÃ©ments concrets sur lesquels il doit raisonner.

Le prompt utilisateur prÃ©cise aussi les attentes rÃ©dactionnelles : rÃ©ponse factuelle, concise, ancrÃ©e dans le champ de lâ€™inclusion et de lâ€™insertion professionnelle.
En cas dâ€™information manquante dans le corpus, le modÃ¨le est invitÃ© Ã  le signaler clairement â€” Ã©vitant ainsi toute hallucination.
Il peut, Ã  titre facultatif, proposer des pistes complÃ©mentaires de recherche ou de vÃ©rification.

Ce dÃ©couplage entre le prompt systÃ¨me (invariants) et le prompt utilisateur (spÃ©cifique Ã  la requÃªte) permet dâ€™obtenir une rÃ©ponse contextualisÃ©e mais disciplinÃ©e, fidÃ¨le Ã  la philosophie du RAG : produire une synthÃ¨se fiable Ã  partir de donnÃ©es internes.

```python
# Prompt user

user_prompt = (
    f"Question : {query}\n\n"
    "Contexte disponible (extraits / documents pertinents) :\n"
    f"{context}\n\n"
    "**Instructions pour la rÃ©ponse :**"  
    "- Donne une rÃ©ponse factuelle, concise et structurÃ©e."
    "- Evite les gÃ©nÃ©ralitÃ©s, les formules vagues ou les rÃ©ponses hors sujet."
    "- Gardes en tÃªte que tu dois toujours envisager ta rÃ©ponse dans le contexte de l'insertion socio-professionnelle et de l'inclusion par l'activitÃ© Ã©conomique."
    "- Bases-toi en prioritÃ© sur les informations prÃ©sentes dans le contexte."
    "- Quand tu cites une information, indique lâ€™identifiant du document (ex. `[Doc3]`, `[Doc7]`).  "
    "- Si une partie de la rÃ©ponse demandÃ©e nâ€™est pas couverte par le contexte, indique clairement : Â« Je nâ€™ai pas trouvÃ© dâ€™information dans les documents fournis concernant â€¦ Â».  "
    "- Si tu peux proposer une piste ou question complÃ©mentaire (sans lâ€™imposer), tu peux lâ€™ajouter Ã  la fin (en prÃ©cisant que câ€™est une suggestion)."
    "\n\n"
    f"RÃ©pond maintenant Ã  la question :  \n**{query}**"
)

```

### 3. RAG

La derniÃ¨re Ã©tape consiste Ã  exÃ©cuter la gÃ©nÃ©ration de la rÃ©ponse Ã  proprement parler.
Câ€™est ici que sâ€™effectue la mise en Å“uvre du mÃ©canisme de RAG (Retrieval-Augmented Generation) : le modÃ¨le est alimentÃ© Ã  la fois par les instructions et par le contexte documentaire issu de la recherche vectorielle.

Le code prÃ©sentÃ© illustre un appel asynchrone Ã  lâ€™API `/api/chat` dâ€™Ollama, qui permet de dialoguer avec un modÃ¨le local open-weight (ici `gpt-oss:20b`).
Le flux de sortie est traitÃ© en continu (*streaming*), ce qui offre une meilleure rÃ©activitÃ© et permet dâ€™afficher la rÃ©ponse au fur et Ã  mesure de sa gÃ©nÃ©ration.
En cas dâ€™erreur rÃ©seau ou de rÃ©ponse vide, des exceptions explicites sont levÃ©es afin de faciliter le diagnostic.

Cette approche simple mais robuste constitue un prototype minimal fonctionnel de pipeline RAG :

1. rÃ©cupÃ©ration du contexte pertinent ;
2. prÃ©paration des prompts ;
3. appel du modÃ¨le ;
4. restitution dâ€™une rÃ©ponse sourcÃ©e et structurÃ©e.

Bien que ce code soit volontairement expÃ©rimental et non industrialisÃ©, il dÃ©montre la faisabilitÃ© dâ€™un chatbot mÃ©tier reposant exclusivement sur des modÃ¨les open weight et des technologies ouvertes, garantissant la maÃ®trise totale des donnÃ©es et du comportement du modÃ¨le.

```python
# Appel de l'API /generate

try:
    answer = await request_ollama_chat(
        [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]
    )
except LLMServiceError as exc:
    raise AnswerGenerationError(str(exc)) from exc

return AskResponse(answer=answer, documents=documents)

```

```python
# DÃ©finition de la mÃ©thode pour appeler l'API /api/chat de Ollama

async def request_ollama_chat(messages: Iterable[Mapping[str, str]]) -> str:
    """Call the Ollama chat endpoint and return the assistant content."""

    payload = {"model": settings.ollama_model, "messages": list(messages)}

    chunks: list[str] = []

    try:
        async with httpx.AsyncClient(timeout=settings.ollama_timeout_seconds) as client:
            async with client.stream(
                "POST",
                f"{settings.ollama_base_url}/api/chat",
                json=payload,
            ) as response:
                response.raise_for_status()

                async for line in response.aiter_lines():
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                    except json.JSONDecodeError:
                        # Ignore malformed chunks, continue reading stream.
                        continue

                    message = data.get("message") if isinstance(data, dict) else None
                    if isinstance(message, dict):
                        content_piece = message.get("content")
                        if isinstance(content_piece, str):
                            chunks.append(content_piece)

                    if data.get("done") is True:
                        break
    except httpx.HTTPStatusError as exc:
        detail = exc.response.text
        raise LLMServiceError(
            f"LLM request failed with status {exc.response.status_code}: {detail}"
        ) from exc
    except httpx.HTTPError as exc:
        raise LLMServiceError("Unable to contact LLM service") from exc

    content = "".join(chunks).strip()
    if not content:
        raise LLMServiceError("LLM response missing assistant content")

    return content

```

> â˜ï¸ Le seul mÃ©rite du code ci-dessus est d'Ãªtre fonctionnel.
> Je l'ai Ã©crit dans le cadre d'un projet de R&D non-industrialisÃ© Ã  des fins de veille technologique et de satisfaction de ma curiositÃ© personnelle.
> Il est trÃ¨s largement perfectible et amÃ©liorable et ne doit surtout pas Ãªtre dÃ©ployÃ© en production en l'Ã©tat !

## IHM

Pour la partie IHM, je ne me suis pas trop pris la tÃªte. 
J'ai demandÃ© Ã  ChatGPT / Codex de me gÃ©nÃ©rer une application Next.js de type Chatbot qui interroge l'API dÃ©veloppÃ©e plus haut.

Le code source est disponible depuis le sous-rÃ©pertoire [/web](https://github.com/jbuget/ia-custom-chatbot/tree/main/web).
Pas grand chose de notable ici.

![Arborescence de l'application web front-end en Next.js avec focus sur l'appel Ã  l'API](webapp.png)


## Conclusion

Nous arrivons au bout de cette aventure et de cet article (ğŸ˜®â€ğŸ’¨ ouf !).

Il y aurait Ã©normÃ©ment de choses Ã  creuser et d'**amÃ©liorations Ã  apporter** pour aller plus loin :

* sÃ©curiser les endpoints, prompts, Ã©crans, points d'accÃ¨s / sortie
* tester diffÃ©rents algorythmes, que ce soit pour la vectorisation / indexation comme des donnÃ©es ou la gÃ©nÃ©ration de rÃ©ponse
* les prompts systÃ¨mes et utilisateurs peuvent aussi Ãªtre trÃ¨s sensiblement amÃ©liorÃ©s
* il faudrait tester et comprarer les rÃ©sultats / temps de rÃ©ponse avec des ressources GPU
* on pourrait aussi comparer avec des technologies telles que ChromaDB ou Pinecone
* en l'Ã©tat, l'UX n'est pas trÃ¨s rÃ©active, et le fait d'attendre la gÃ©nÃ©ration complÃ¨te de la rÃ©ponse ne donne pas "confiance" ou "envie". On pourrait mettre en place du Server-Side-Event ou des Web Sockets pour dynamiser le rendu
* telle qu'elle est implÃ©mentÃ©e, l'application (front et back) ne tient pas compte des sessions de discussion. On pourrait supporter le multi-messages pour tenir de la mÃ©moire / contexte de toute la discussion pour gÃ©nÃ©rer une meilleure rÃ©ponse
* et plein d'autres choses encoreâ€¦


Au-delÃ  du code, **jâ€™ai appris pas mal de choses essentielles** dans ce vaste far-west que reprÃ©sente lâ€™IA Ã  lâ€™heure actuelle :

* Ce que jâ€™ai enfin compris : le principe dâ€™*embedding* et la recherche vectorielle â€” ce nâ€™est pas de la magie, juste une autre faÃ§on dâ€™exprimer la proximitÃ© entre les idÃ©es ;
* Ce que jâ€™ai aimÃ© explorer : diffÃ©rents runtimes et modÃ¨les open-source/weight (Ollama, vLLM, `gpt-oss:20b`, `nomic-embed-text-v2-moe`, etc.), chacun avec ses forces et ses limites ;
* Ce que jâ€™ai dÃ©couvert : quâ€™Ollama sait faire bien plus que de la gÃ©nÃ©ration de texte (notamment lâ€™embedding), mÃªme sâ€™il reste parfois limitÃ© sur certains modÃ¨les rÃ©cents ;
* Ce qui mâ€™a surpris : manipuler des frameworks comme PyTorch ou Sentence Transformers, câ€™est finalement assez abordable ;
* Et enfin : aprÃ¨s dix ans de Java puis dix ans de JS/TS/Node.js, jâ€™ai vraiment pris plaisir Ã  mettre les mains dans Python (et ce n'Ã©tait pas gagnÃ© ğŸ¤£)

Prochaine Ã©tape (mais pas tout de suite ğŸ«©) : pousser plus loin ce qu'il est possible de faire avec l'IA, notamment en jouant avec des ressources GPU.

Que tu sois un(e) humain(e) ou un bot, merci pour le courage et la patience de m'avoir lu jusque lÃ  ğŸ¤— !