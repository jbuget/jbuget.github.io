---
date: 2025-10-15T09:02:00+02:00
draft: false
params:
  author: J√©r√©my Buget
title: D√©velopper un agent IA custom
categories:
  - IA
  - R&D
  - D√©veloppement
tags:
  - Chatbot
  - LLM
  - RAG
  - Ollama
  - IA open-source
  - FastAPI
  - Python
  - pgvector
description: >
  Retour d‚Äôexp√©rience sur la conception d‚Äôun chatbot m√©tier bas√© sur un corpus priv√©, 
  en combinant vectorisation, recherche s√©mantique et g√©n√©ration de r√©ponses avec des mod√®les open-weight (Ollama, Sentence Transformers, FastAPI, PostgreSQL).
cover:
  image: "chatbot_inclusion_pmsmp-definition.png"
  alt: "Capture d‚Äô√©cran du chatbot IA d√©di√© √† l‚Äôinclusion socio-professionnelle"
  caption: "Chatbot IA ‚Äì D√©monstration d‚Äôun agent RAG sur corpus m√©tier"
  relative: true
meta:
  ogType: "article"
  ogImage: "chatbot_inclusion_pmsmp-definition.png"
  twitterCard: "summary_large_image"
  twitterCreator: "@jbuget"
  keywords: ["chatbot", "IA", "open-weight", "RAG", "Ollama", "FastAPI", "pgvector", "Sentence Transformers"]

---

## Table des mati√®res
- [Introduction](#introduction)
- [TL;DR](#tldr)
- [Objectifs](#objectifs)
- [Cas d'usage](#cas-dusage)
- [Conditions op√©rationnelles et mat√©rielles](#conditions-op√©rationnelles-et-mat√©rielles)
- [Collecte de donn√©es](#collecte-de-donn√©es)
- [Indexation vectorielle](#indexation-vectorielle)
  - [1. Choix du syst√®me de gestion de base de donn√©es (SGBD)](#1-choix-du-syst√®me-de-gestion-de-base-de-donn√©es-sgbd)
  - [2. Cr√©ation de la base + table](#2-cr√©ation-de-la-base--table)
  - [3. Indexation des documents](#3-indexation-des-documents)
    - [3.1) Vectorisation des topics](#31-vectorisation-des-topics)
    - [3.2) Insertion des documents en base](#32-insertion-des-documents-en-base)
- [Conception de l'API](#conception-de-lapi)
- [Recherche s√©mantique par comparaison vectorielle](#recherche-s√©mantique-par-comparaison-vectorielle)
- [G√©n√©ration de la r√©ponse](#g√©n√©ration-de-la-r√©ponse)
  - [1. Instructions syst√®me](#1-instructions-syst√®me)
  - [2. Instructions utilisateur](#2-instructions-utilisateur)
  - [3. RAG](#3-rag)
- [IHM](#ihm)
- [Conclusion](#conclusion)


## Introduction

Comme tout le monde, j'ai int√©gr√© et continue d'incorporer de plus en plus de pratiques, raisonnements et outils IA dans mon quotidien, pro ou perso : **agents conversationnels** (ChatGPT, Gemini) pour comprendre des choses et g√©n√©rer / am√©liorer du contenu ; **compagnons techniques** (Codex, Claude Code, Zed) pour m'aider √† produire et g√©rer du code ; **algorithmes et mod√®les** (LLM, OCR, STT/TTS) pour r√©aliser des t√¢ches relativement compliqu√©es ou r√©barbatives comme extraire, modifier ou transformer des informations sous toutes les formes.

Savoir utiliser cette technologie et ce qu'elle implique me para√Æt d√©sormais une comp√©tence professionnelle essentielle, quel que soit le m√©tier en question, relatif √† l'informatique ou non. En tant que d√©veloppeur, j'ai cherch√© √† aller un cran plus loin et explor√© comment concevoir et produire un outil exploitant l'IA *dans* et *pour* un domaine fonctionnel particulier.

C'est ainsi que j'ai r√©alis√© un petit chatbot capable d'interroger un corpus pr√©d√©fini de documents sp√©cifiques pour √©laborer des r√©ponses sourc√©es, dans la limite du champs d√©fini.

## TL;DR

- J'ai d√©velopp√© un chatbot IA sp√©cialis√© dans le domaine de l'inclusion socio-professionnelle
- Cet agent se base sur un corpus de documents issus du site [La communaut√© de l'inclusion](https://communaute.inclusion.gouv.fr/)
- J'ai d√©marr√© un serveur Ollama avec le mod√®le open-weight de OpenAI : `gpt-oss:20b`
- J'ai cod√© **un script de crawling** (en Node.js, avec Cheerio.js) pour r√©cup√©rer des fiches d'information
- J'ai install√© **une base de donn√©es PostgreSQL** et activ√© l'extension **pgvector**
- J'ai d√©clar√© une base avec une table disposant d'une colonne de type `embedding VECTOR(768)`
- J'ai vectoris√© ces fiches gr√¢ce √† la lib python **Sentence Transformers** (768 dimensions) et au mod√®le `nomic-ai/nomic-embed-text-v2-moe`
- J'ai index√© les embeddings obtenus dans la base PG
- J'ai d√©velopp√© **une API (Python/FastAPI)** pour mobiliser les diff√©rentes briques IA (query + RAG) sollicit√©es
- Lorsqu'un utilisateur interroge le chatbot, son prompt est √† son tour vectoris√© pour effectuer une recherche par comparaison de vecteurs. On obtient ainsi un ensemble de "documents sources" correspondants (avec un pourcentage de pertinence)
- Je passe alors ces documents comme √©l√©ments exclusifs de "contexte" dans l'instruction que soumets au **LLM responsable de la g√©n√©ration de la r√©ponse** (en interrogeant `/api/chat` de l'API expos√©e par Ollama)
- Je passe aussi au LLM les autres (pr√©c√©dents) messages de la conversation en cours
- J'ai d√©velopp√© **une webapp chatbot-like** toute simple pour poser des questions et afficher les r√©ponses
- Le code source est disponible sur GitHub : [jbuget/ia-custom-chatbot](https://github.com/jbuget/ia-custom-chatbot)

![Capture d'√©cran du chatbot (question + r√©ponse)](chatbot_inclusion_pmsmp-definition.png)


## Objectifs

Mon objectif, √† travers ce POC (proof of concept), √©tait de **comprendre comment exploiter l'IA d'un point de vue v√©ritablement m√©tier, plus seulement comme outil de production ou aide √† la cr√©ation**. Je souhaitais aussi d√©couvrir les solutions envisageables, √©valuer leur potentiel et me confronter √† leurs limites / contraintes.

Concr√®tement, je voulais concevoir une API / webapp capable de d'interagir (√©couter et r√©pondre) en langage naturel, avec des utilisateurs sp√©cifiques d'un domaine, pour r√©pondre le plus pr√©cis√©ment √† des questions m√©tier, en s'appuyant et en citant des sources que j'aurais au pr√©alable collect√©es et index√©es.

## Cas d'usage

J'ai pas mal cherch√© sur quel champs fonctionnel baser le projet.

Ayant beaucoup ≈ìuvr√© dans le service public ces derni√®res ann√©es, j'ai d'abord creus√© du c√¥t√© de [data.gouv.fr](https://data.gouv.fr). S'il y a √©norm√©ment de donn√©es rendues accessibles par les administrations, les collectivit√©s et leurs partenaires, la plupart des datasets sont des donn√©es fortement structur√©es, en colonnes. Pour mon cas d'usage, je souhaitais de la donn√©es moins structur√©es, √† base de contenu et textes riches.

Je me suis tourn√© vers les sites et plateformes de datasets, comme [Kaggle](https://www.kaggle.com/datasets). Mais l√† encore, c'√©tait beaucoup de donn√©es structur√©es, de qualit√© √† g√©om√©trie vraiment variable.

Je pense qu'on retrouve ce type de donn√©es pr√©cieuses en entreprise, et qu'il faut pouvoir les int√©grer et en tenir compte lors de requ√™tes utilisateurs, mais dans le cadre de mon projet d'IA, je voulais vraiment mettre l'accent sur l'exploitation de documents bruts.

Finalement, j'ai eu l'id√©e de m'appuyer sur les fiches d'information publi√©es sur le site de [La communaut√© de l'inclusion](https://communaute.inclusion.gouv.fr), pour **proposer un agent conversationnel expert dans le domaine de l'insertion socio-professionnelle**.

J'ai cherch√© √† obtenir un chatbot capable de r√©pondre √† des questions sur l'IAE, les PMSMP et d'aider les professionnels de l'inclusion dans leur accompagnement de demandeurs d'emploi.

![La communaut√© de l'inclusion](communaute_inclusion.png)


## Conditions op√©rationnelles et mat√©rielles

J'ai con√ßu, d√©velopp√© et ex√©cut√© le programme exclusivement en local sur ma machine.

> üíª Il s'agit d'un Macbook Pro M2 Max, 64Go RAM, 12 c≈ìurs, sans carte graphique / GPU.

Je me suis fix√© comme contrainte de n'utiliser **que des mod√®les open-weight et des logiciels / briques open-source**.

J'ai ainsi utilis√© [Ollama](https://ollama.com/) comme serveur de mod√®les IA ouverts (thinking, embedding, vision, etc.).


## Collecte de donn√©es

La communaut√© de l'inclusion est une plateforme communautaire en ligne d'espaces de discussion sur des sujets relatifs √† l'insertion socio-professionelle. On y trouve des "forums" et des "topics". On y trouve surtout des "fiches pratiques" officielles, √©dit√©es et maintenues par la Plateforme de l'inclusion (PDI), de tr√®s grande qualit√© (fond, forme, fra√Æcheur).

Le point de d√©part de mon projet IA a donc consist√© √† r√©cup√©rer ces fameuses fiches d'information. Malheureusement, la PDI ne diffuse pas ces informations en open content sur data.gouv.fr.

Je n'ai pas eu d'autre choix que de d√©velopper un petit programme (en Node.js / TypeScript) pour crawler/scrapper les donn√©es directement depuis le site web. Les sources sont disponibles sur GitHub : [jbuget/crawler](https://github.com/jbuget/meilisearch-crawler/blob/main/src/crawler.ts).

Je me suis content√© de r√©cup√©rer les fiches pratiques, pas les questions / r√©ponses, car beaucoup de questions ne sont pas r√©ellement pertinentes ou directement corr√©l√©es au domaine, contiennent des donn√©es personnelles, ne sont pas r√©pondues ou sont des redites ou pointeurs vers les fiches pratiques.

C'est ainsi que j'ai obtenu un fichier JSON avec les URLs des fiches pratiques et le contenu textuel (HTML ‚Üí texte) des pages (merci Cheerio.js). Soit un corpus, en fran√ßais, de pr√®s d'un millier de documents.

```JSON
// data/topics.json

[
  {
    "id": "12513c26d6e647fc537447aad16bd9337988933bd44f0517cd3c0767b3f9bde0",
    "url": "https://communaute.inclusion.gouv.fr/forum/fiche-pratique-cellule-alerte-inclusion-163/",
    "title": "Fiche pratique Cellule-alerte-inclusion",
    "subtitle": "Vous √™tes intervenant social ou b√©n√©vole et vous estimez qu‚Äôune personne que vous suivez devrait b√©n√©ficier du dispositif de plafonnement des frais bancaires ou de l‚Äôoffre sp√©cifique client√®le fragile ? Vous accompagnez une personne d√©pourvue de compte bancaire ? Vous pouvez saisir la cellule alerte inclusion.",
    "content": "- lien vers la page de pr√©sentation sur banque-france.fr\n\n- lien vers le flyer de pr√©sentation"
  },
  {
    "id": "aa946e302c55270dffd388b3f289a443727651ad69df3c47d9adc3bcbddd9f09",
    "url": "https://communaute.inclusion.gouv.fr/forum/m%C3%A9mo-de-vie-prot%C3%A9ger-vos-documents-et-vos-t%C3%A9moignages-162/",
    "title": "M√©mo de vie - Prot√©ger vos documents et vos t√©moignages",
    "subtitle": "La plateforme M√©mo de Vie est destin√©e √† toute personne victime de violences sans distinction de genre, ni de sexe et en questionnement sur des violences subis.",
    "content": "lien vers memo-de-vie.org"
  },
  {
    "id": "e7ff95de2617b05b7e1ed0fbe059d5436029e8c4a964b782e5a8ef2e5deb6939",
    "url": "https://communaute.inclusion.gouv.fr/forum/outils-de-pr%C3%A9vention-prostitution-jeunes-161/",
    "title": "Outils de Pr√©vention Prostitution Jeunes",
    "subtitle": "Con√ßu par l‚ÄôAmicale du Nid. Ses objectifs : sensibiliser les jeunes √† la question de la prostitution, pr√©venir les risques prostitutionnels, informer sur les droits et le fait qu‚Äôun accompagnement est possible pour sortir de la prostitution.",
    "content": "lien vers la m√©diath√®que jenesuispasavendre.org"
  },
  ...
]
```


## Indexation vectorielle

Une fois la collecte des donn√©es r√©alis√©es, il a fallu les d√©verser de fa√ßon appropri√©e "quelque part" (spoiler alert : dans une base de donn√©es).

### 1. Choix du syst√®me de gestion de base de donn√©es (SGBD)

Ces derniers temps, j'ai beaucoup jou√© avec **Typesense** ou **Meilisearch**. Ces deux solutions sont des moteurs de recherche (textuelle) int√©grant de la recherche vectorielle pour des usages IA. Il existe des bases de donn√©es sp√©cialis√©es dans la recherche vectorielle comme **ChromaDB** (open-source, self-hostable ou SaaS), **Pinecone** (SaaS), **Qdrant** (open-source, self-hostable ou SaaS), **Weaviate** (open-source, self-hostable ou SaaS) ou **Milvus** (open-source, self-hostable ou SaaS).

Dans la mesure o√π j'ai relativement peu de documents et que mon serveur (c-√†-d ma machine) ne dispose pas de capacit√©s GPU, j'ai pr√©f√©r√© opter pour la solution **PostgreSQL + extension `pgvector`**, a priori adapt√©e pour des volumes et besoins de taille moyenne.

### 2. Cr√©ation de la base + table

√Ä l'initialisation de la base (script init.sql), j'ai commenc√© par activer l'extension pgvector.

Puis, j'ai cr√©√© la table "topics" avec une colonne "embedding".
Dans la mesure o√π j'ex√©cute le syst√®me sur mon poste, j'ai opt√© pour une configuration moyenne √† 768 dimensions.
Il semble pertinent de cibler 1536 dimensions pour du requ√™tage plus fin.

> ‚ö†Ô∏è Il faut bien faire attention √† retenir ce dimensionnement (768) lors du requ√™tage de donn√©es c√¥t√© back-end.

Enfin, j‚Äôai d√©clar√© un index IVFFLAT `idx_topics_embedding` pour acc√©l√©rer la recherche vectorielle.

```sql {hl_lines=[14]}
-- data/init.sql

CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE topics (
    id bigserial PRIMARY KEY,  
    embedding VECTOR(768), 
    title TEXT,
    subtitle TEXT,
    content TEXT,
    url TEXT UNIQUE
);

CREATE INDEX idx_topics_embedding ON topics USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);
```

L‚Äôextension `pgvector` supporte plusieurs types d‚Äôindex pour la recherche de similarit√©, dont les deux plus courants sont `ivfflat` (Inverted File Flat) et `hnsw` (Hierarchical Navigable Small World Graph).

* IVFFLAT est plus adapt√© pour des volumes de donn√©es moyens (quelques milliers √† centaines de milliers d‚Äô√©l√©ments) ou quand on dispose de ressources plus limit√©es (m√©moire, espace disque) ; √† noter que le temps d'indexation est plus r√©duit ;
* HNSW est plus adapt√© pour les larges jeux de donn√©es, pour des donn√©es qui changent r√©guli√®rement, pour des recherches plus rapides et lorsque l'on dispose de ressources sup√©rieures (ex : GPU)

> üí° Si on envisage de manipuler des millions de vecteurs ou de faire √©voluer r√©guli√®rement la base, HNSW serait un meilleur choix. > Pour des projets exploratoires ou des d√©monstrations, comme c'est mon cas ici, IVFFLAT reste la solution la plus pragmatique et l√©g√®re.

Pour aller plus loin, je vous invite √† lire ce billet : "[PGVector: HNSW vs IVFFlat ‚Äî A Comprehensive Study](https://medium.com/@bavalpreetsinghh/pgvector-hnsw-vs-ivfflat-a-comprehensive-study-21ce0aaab931)".

### 3. Indexation des documents

L'indexation d'un document dans la base de donn√©es se passe comme suit :

* on ex√©cute un calcul de *vectorisation* (via un LLM) du contenu textuel du document
* on ins√®re dans la table topics une nouvelle entr√©e avec les champs structur√©s `url`, `title`, `subtitle` ainsi que le champs vectoris√© `embedding` 
* le requ√™tage de documents se fait alors en comparant le vecteur du prompt (obtenu via le m√™me mod√®le, √† la vol√©e) et les diff√©rents vecteurs stock√©s en base

C'est ici que j'ai rencontr√© des soucis de pertinence des r√©sultats et qu'il m'a fallu consid√©rer deux approches / types de mod√®les de vectorisation.


#### 3.1) Vectorisation des topics

Dans un premier temps, j'ai fait appel √† l'API de Ollama [`/api/embeddings`](https://docs.ollama.com/api#generate-embedding). ‚Äì dont le mod√®le sous-jacent par d√©faut est [`nomic-embed-text:v1.5`](https://ollama.com/library/) ‚Äì pour obtenir des vecteurs (a.k.a. "embeddings") de chaque document (rappel : des *topics*). √áa a sembl√© fonctionn√© techniquement, mais √† l'usage (requ√™tage), je trouvais les r√©sultats insatisfaisants. 

Certaines questions invoquaient les bons documents, mais beaucoup d'autres, notamment celles tournant autour d'acronymes ("PMSMP", "IAE", "CIP"), ne remontaient aucun documents correspondants. Les r√©ponses formul√©es par la partie RAG (cf. ci-dessous) n'√©taient donc pas tr√®s pertinentes ni exploitables pour de vrais utilisateurs. "Shit in, shit out".

J'ai tent√© avec plusieurs dimensionnements (1536) et mod√®les d'embedding propos√©s par Ollama (comme `mxbai-embed-large` (mars 2024)), mais aucun ne me donnait satisfaction. J'ai alors d√©cid√© de g√©n√©rer moi-m√™me les vecteurs de chaque topic.

> üí° En r√©digeant cet article, je d√©couvre que Ollama propose maintenant un mod√®le d'embedding plus puissant (et un peu plus lourd) : [bge-m3](https://ollama.com/library/bge-m3). Peut-√™tre aurait-il fait l'affaire.

Pour cela, j'ai utilis√© la biblioth√®que Sentence Transformers (a.k.a. [SBERT](https://www.sbert.net/)), avec l'√©volution du mod√®le standard de Ollama : [`nomic-embed-text-v2-moe`](todo), lequel impl√©mente le concept de [Mixtures of Experts](https://huggingface.co/blog/moe).

```python
# data/load_topics_with_embeddings.py

model = SentenceTransformer(
            model_name="nomic-ai/nomic-embed-text-v2-moe",
            device="cpu",
            trust_remote_code="true",
        )

text = [
    part.strip()
    for part in (
        topic.get("title", ""),
        topic.get("subtitle", ""),
        topic.get("content", ""),
    )
    if isinstance(part, str) and part.strip()
]

vector = model.encode(
            text,
            convert_to_numpy=True,
            normalize_embeddings=False,
            show_progress_bar=False,
        ).tolist()

# ...

return [float(value) for value in vector]
```

> üí° Lors du tout premier appel √† Sentence Transformers, celui-ci doit t√©l√©charger le mod√®le indiqu√© dans le client, ce qui peut prendre quelques minutes.

C'est ainsi que pour chaque topic, j'ai obtenu un vecteur propre calcul√© depuis les propri√©t√©s concat√©n√©es `title` + `subtitle` et `content` :

```python
# Input
title : "Fiche pratique Cellule-alerte-inclusion"
subtitle : "Vous √™tes intervenant social ou b√©n√©vole et vous estimez qu‚Äôune personne que vous suivez devrait b√©n√©ficier...""
content : "- lien vers la page de pr√©sentation sur banque-france.fr\n\n- lien vers le flyer de pr√©sentation..."

# Output
embedding : [-0.0030780921,-0.03730278,0.0012224227,0.001161514,-0.008647267,0.0004938656,-0.036171958,0.02440677,... (x760)]
```

#### 3.2) Insertion des documents en base

Une fois les vecteurs calcul√©s, il ne restait plus qu'√† enregistrer les donn√©es en base, avec un simple `INSERT INTO ... VALUES` :

```python
# data/load_topics_with_embeddings.py

def reset_and_insert_topics(
    conn: psycopg.Connection, payload: list[tuple[object, object, object, object, object]]
) -> int:
    with conn.cursor() as cur:
        cur.execute("TRUNCATE TABLE topics RESTART IDENTITY CASCADE")
        if payload:
            cur.executemany(
                """
                INSERT INTO topics (title, subtitle, content, url, embedding)
                VALUES (%s, %s, %s, %s, %s)
                """,
                payload,
            )

    return len(payload)
```

Nous obtenons alors ce type d'objets en base : 

```shell
chatbot=> select id, url, title, embedding from topics where url='https://communaute.inclusion.gouv.fr/forum/fiche-pratique-cellule-alerte-inclusion-163/';

-[ RECORD 1 ]-
id          | 47
url         | https://communaute.inclusion.gouv.fr/forum/fiche-pratique-cellule-alerte-inclusion-163/
title       | Fiche pratique Cellule-alerte-inclusion
subtitle    | Vous √™tes intervenant social ou b√©n√©vole et vous estimez qu‚Äôune personne que vous suivez devrait b√©n√©ficier...
embedding   | [-0.0030780921,-0.03730278,0.0012224227,0.001161514,-0.008647267,0.0004938656,-0.036171958,0.02440677,... (x760)]
```

Tout √©tait pr√™t d√©sormais pour passer √† la suite : d√©velopper une API exploitant la recherche vectorielle et formulant une r√©ponse RAG sur la base et limit√©e aux documents r√©sultants.

## Conception de l'API

√Ä ce stade, je poss√©dais une base avec des donn√©es pr√™tes √† √™tre requ√™t√©es et pass√©es en contexte d'instructions donn√©es √† un LLM, charg√© lui de g√©n√©rer une r√©ponse √† l'utilisateur, en langage naturel, sur la base des documents remont√©s.

J'ai commenc√© par initialiser un projet Python / FastAPI, qui expose le endpoint `POST /api/ask`.

Son fonctionnement est le suivant :

1. on extrait les param√®tres de la requ√™tes HTTP, en particulier la question (phrase)
2. on vectorise la question, avec le m√™me algo / mod√®le utilis√© pour indexer les topics
3. on effectue une recherche des topics par comparaison vectorielle avec celui de la question
4. on g√©n√®re une r√©ponse via un second LLM en lui passant en contexte les documents remont√©s
5. on formatte et renvoie un objet JSON dans la r√©ponse HTTP

Cette route prend en param√®tres :

- `question` : la question utilisateur √† interpr√©ter
- `top_k` (optionnel) : le nombre maximum de documents √† citer.

```shell
# Request HTTP

curl --request POST \
  --url http://localhost:8000/api/v1/ask \
  --header 'Content-Type: application/json' \
  --data '{
	"question": "Qu'\''est-ce qu'\''une PMSMP ?",
	"top_k": "5"
}'
```

Elle retourne un objet JSON avec les champs :

- `answer` : la r√©ponse formul√©e par l'assistant
- `documents` : les documents cit√©s pour appuyer la r√©ponse

```shell
# Response HTTP (extrait)

{
	"answer": "**Qu‚Äôest‚Äëce qu‚Äôune PMSMP‚ÄØ?**\n\n- **P√©riode de Mise en Situation en Milieu Professionnel (PMSMP)**  \n  Il s‚Äôagit d‚Äôun dispositif d‚Äôimmersion professionnelle propos√© √† toute personne, quel que soit son statut ou son √¢ge, lorsqu‚Äôelle b√©n√©ficie d‚Äôun accompagnement social ou professionnel personnalis√©‚ÄØ:  \n  - personnes sans activit√© en parcours d‚Äôinsertion (recherche d‚Äôemploi, jeunes accompagn√©s par la Mission locale, b√©n√©ficiaires du RSA, etc.)  \n  - personnes en activit√© engag√©es dans une d√©marche d‚Äôinsertion ou de r√©orientation professionnelle (salari√©s en r√©orientation, accompagn√©s par une SIAE, salari√©s en CUI, etc.).  \n  Cette p√©riode permet √† la personne de vivre une exp√©rience concr√®te dans un environnement de travail r√©el, afin d‚Äôidentifier ses forces, ses besoins de formation et d‚Äôenrichir son projet d‚Äôemploi.  \n  *(source‚ÄØ:‚ÄØ[Doc1])*\n\n- **Cadre de l‚Äôexp√©rience accompagn√©e**  \n  Dans le mod√®le d‚Äôaccompagnement vers l‚Äôemploi ¬´‚ÄØexp√©rience accompagn√©e‚ÄØ¬ª, la PMSMP constitue l‚Äôun des outils cl√©s. Elle est combin√©e aux Actions de Formation en Situation de Travail (AFEST) et √† la Pr√©paration Op√©rationnelle √† l‚ÄôEmploi (POEI / POEC) pour offrir des immersions r√©elles et des occasions d‚Äôapprentissage sur le terrain.  \n  *(source‚ÄØ:‚ÄØ[Doc2])*\n\nEn r√©sum√©, une PMSMP est une p√©riode d‚Äôimmersion professionnelle destin√©e √† aider les personnes √©loign√©es de l‚Äôemploi √† tester, acqu√©rir et valoriser des comp√©tences dans un cadre r√©el, dans le cadre d‚Äôun accompagnement individualis√©.",
	"documents": [
		{
			"rank": 1,
			"topic_id": 68,
			"title": "La PMSPM : P√©riode de Mise en Situation en Milieu Professionnel",
			"url": "https://communaute.inclusion.gouv.fr/forum/la-pmspm-p%C3%A9riode-de-mise-en-situation-en-milieu-professionnel-124/",
			"excerpt": "- Les PMSMP sont ouvertes sur prescription √† toute personne quel que soit son statut ou son √¢ge, d√®s lors qu'elle fait l'objet d'un‚Ä¶",
			"similarity": 0.6959141391237493
		},
		{
			"rank": 2,
			"topic_id": 87,
			"title": "ü§ùüë©L'exp√©rience accompagn√©e des publics √©loign√©s de l'emploi",
			"url": "https://communaute.inclusion.gouv.fr/forum/lexp%C3%A9rience-accompagn%C3%A9e-des-publics-%C3%A9loign%C3%A9s-de-lemploi-190/",
			"excerpt": "L'exp√©rience accompagn√©e est une m√©thode d'accompagnement vers l'emploi qui propose un changement de paradigme par rapport aux approches‚Ä¶",
			"similarity": 0.5818639535972785
		},
		{ ... },
		{ ... },
		{ ... }
	]
}
```

## Recherche s√©mantique par comparaison vectorielle

> üí° Par souci de lisibilit√©, j'ai volontairement r√©duit et simplifi√© le code ci-dessous. Pour rappel, tout le code est disponible [sur mon GitHub](https://github.com/jbuget/ia-custom-chatbot/).

En premier lieu, on calcule "l'embedding" ou "vecteur" de la question. 

Pour que la comparaison s√©mantique entre la question et les documents index√©s en base de donn√©es puisse se faire, **il faut s'assurer d'utiliser le m√™me mod√®le de calcul d'embedding**, ici `nomic-ai/nomic-embed-text-v2`.

On obtient un tableau de 768 valeurs flottantes, soit le nombre de dimensions d√©finies pour le champs embeddings de la table topics.

```python
# Calcul du vecteur de la question

model = await asyncio.to_thread(
    SentenceTransformer,
    "nomic-ai/nomic-embed-text-v2",
    "cpu",
    trust_remote_code=True,
)

vector = await asyncio.to_thread(
    model.encode,
    text,
    show_progress_bar=False,
    convert_to_numpy=True,
    normalize_embeddings=False,
)

values = vector.tolist()

embedding = [float(value) for value in values]

return embedding
```

Une fois cet embedding obtenu, il suffit de pr√©parer et ex√©cuter une requ√™te SQL qui trie les documents correspondant √† la question par rapport √† leur "proximit√©" ou "similarit√©".

**La m√©trique de similarit√© utilis√©e ici pour effectuer la comparaison est la `distance cosine`.**

> Il existe plusieurs m√©triques de similarit√© utilis√©es pour comparer des vecteurs selon le type d‚Äôapplication :
> - **la similarit√© cosinus** (utilis√©e ici, via l'op√©rateur `<=>`) mesure l‚Äôangle entre deux vecteurs et est couramment utilis√©e pour la recherche s√©mantique sur du texte, car elle √©value la proximit√© de sens ind√©pendamment de la longueur des vecteurs.
> - **la distance euclidienne (L2)** (via l'op√©rateur `<->`) calcule la distance g√©om√©trique directe entre les points et convient mieux √† des cas o√π la position ou la magnitude ont du sens, comme la recherche g√©ospatiale ou les donn√©es physiques.
> - **le produit scalaire (Inner Product)** tient compte √† la fois de l‚Äôorientation et de la taille des vecteurs, ce qui le rend particuli√®rement adapt√© aux syst√®mes de recommandation, o√π l‚Äôintensit√© d‚Äôune pr√©f√©rence ou d‚Äôune relation est importante.

```python
# Requ√™tage de la BDD

pool = get_pool()
    
vector = f"[{", ".join(f"{value:.10f}" for value in embedding)}]"

async with pool.connection() as conn:
    async with conn.cursor(row_factory=dict_row) as cursor:
        await cursor.execute(
            """
            SELECT
                id,
                title,
                subtitle,
                content,
                url,
                1 / (1 + (embedding <=> %s)) AS similarity
            FROM topics
            WHERE embedding IS NOT NULL
            ORDER BY embedding <=> %s
            LIMIT %s
            """,
            (vector, vector, limit),
        )
        rows = await cursor.fetchall()

        if rows:
            return rows

return rows
```

C'est ainsi qu'on se retrouve avec un tableau de `top_k` documents correspondant s√©mantiquement √† la `question` pos√©e.

## G√©n√©ration de la r√©ponse

Maintenant que nous disposons des topics susceptibles de fournir une r√©ponse √† notre question, nous pouvons les utiliser pour g√©n√©rer une r√©ponse formelle en langage naturel.
Pour cela, nous utilisons un second mod√®le LLM, dit de raisonnement et g√©n√©ration de contenu textuel.

Pour l'exp√©rience, j'ai test√© plusieurs LLMs open weight.
J'ai obtenu les meilleurs par rapport √† ma machine et au temps acceptable (60s) avec `gpt-oss:20b`.

### 1. Instructions syst√®me

Les *instructions syst√®me* d√©finissent le r√¥le du mod√®le et le cadre g√©n√©ral dans lequel il doit op√©rer.
On peut les voir comme la personnalit√© ‚Äúsquelette‚Äù du mod√®le ‚Äî ce qui va orienter son comportement quel que soit le sujet.
Elles jouent le r√¥le de "personnalit√©" et de garde-fou conceptuel du LLM : elles lui rappellent ce qu‚Äôil est cens√© faire, comment il doit r√©pondre, et surtout ce qu‚Äôil ne doit pas faire.

Dans notre cas, le mod√®le est explicitement positionn√© comme un assistant expert du domaine de l‚Äôinsertion socio-professionnelle et des politiques publiques associ√©es.
Ces consignes garantissent que le mod√®le reste centr√© sur le p√©rim√®tre m√©tier et qu‚Äôil s‚Äôexprime en fran√ßais, de mani√®re rigoureuse, structur√©e et sourc√©e.

Elles imposent √©galement plusieurs contraintes essentielles :

* Limiter la r√©ponse aux seules donn√©es issues du corpus interne (les fiches documentaires).
* Citer les identifiants des documents utilis√©s ([Doc1], [Doc2], etc.), afin d‚Äôassurer la tra√ßabilit√© et la v√©rifiabilit√© des r√©ponses.
* Signaler explicitement les lacunes ou les incertitudes du corpus, au lieu de produire des approximations.

L‚Äôobjectif de ce prompt syst√®me est donc de verrouiller le comportement du mod√®le pour qu‚Äôil reste un agent d‚Äôanalyse documentaire fiable, et non un g√©n√©rateur de texte g√©n√©raliste.

```python
# Prompt syst√®me

system_prompt = (
    "Vous √™tes un assistant expert sp√©cialis√© dans le milieu de l‚Äôinsertion socio‚Äëprofessionnelle, √† l‚Äôaccompagnement des personnes √©loign√©es de l‚Äôemploi, et aux dispositifs publics en France (ex. PMSMP, accompagnement, dispositif public, prestataires, droits, obligations).\n"
    "Vous devez :\n"
    "1. R√©pondre **en fran√ßais**, de fa√ßon claire, factuelle, structur√©e (paragraphes, listes si utile).\n"
    "2. Ne mentionner dans votre r√©ponse que les informations **strictement issues des documents de la base** (les fiches scrapp√©es).\n"
    "3. Chaque fois que vous citez une donn√©e / r√®gle / information provenant d‚Äôune fiche, indiquer explicitement son identifiant (ex. `[Doc12]`, `[Doc5]`).\n"
    "4. Si une question demande une information **non pr√©sente dans les documents**, l‚Äôindiquer clairement, de sorte que l‚Äôutilisateur sache que la source n‚Äôa pas fourni cette r√©ponse.\n"
    "5. Ne pas halluciner : ne pas inventer des dispositifs, articles ou chiffres non pr√©sents dans vos documents, sauf si vous avez la certitude (et toujours en pr√©cisant la source).\n"
    "6. Si la question porte sur une mise √† jour r√©cente (loi, jurisprudence) ou une zone d‚Äôincertitude, vous pouvez signaler les limites, et recommander √† l‚Äôutilisateur de v√©rifier les textes officiels ou sources actualis√©es."
    "\n\n"
    "M√™me si aucune r√©ponse exacte n‚Äôest disponible, propose des √©l√©ments proches ou des d√©marches pour trouver l‚Äôinformation recherch√©e.\n"
    "\n\n"
    "**Objectif :** servir de ‚Äúpoint de v√©rit√©‚Äù extrait des fiches de la ‚ÄúCommunaut√© de l‚ÄôInclusion‚Äù, et aider l‚Äôutilisateur √† approfondir ses recherches via ces documents internes.\n" 
)
```

### 2. Instructions utilisateur

Les *instructions utilisateur* servent √† contextualiser la requ√™te ponctuelle adress√©e au mod√®le et √† guider la g√©n√©ration de la r√©ponse.
Elles traduisent la question de l‚Äôutilisateur (`query`) et le contexte documentaire pertinent (`context`) issu de la phase de recherche s√©mantique.
Autrement dit, c‚Äôest √† ce stade que le mod√®le re√ßoit les √©l√©ments concrets sur lesquels il doit raisonner.

Le prompt utilisateur pr√©cise aussi les attentes r√©dactionnelles : r√©ponse factuelle, concise, ancr√©e dans le champ de l‚Äôinclusion et de l‚Äôinsertion professionnelle.
En cas d‚Äôinformation manquante dans le corpus, le mod√®le est invit√© √† le signaler clairement ‚Äî √©vitant ainsi toute hallucination.
Il peut, √† titre facultatif, proposer des pistes compl√©mentaires de recherche ou de v√©rification.

Ce d√©couplage entre le prompt syst√®me (invariants) et le prompt utilisateur (sp√©cifique √† la requ√™te) permet d‚Äôobtenir une r√©ponse contextualis√©e mais disciplin√©e, fid√®le √† la philosophie du RAG : produire une synth√®se fiable √† partir de donn√©es internes.

```python
# Prompt user

user_prompt = (
    f"Question : {query}\n\n"
    "Contexte disponible (extraits / documents pertinents) :\n"
    f"{context}\n\n"
    "**Instructions pour la r√©ponse :**"  
    "- Donne une r√©ponse factuelle, concise et structur√©e."
    "- Evite les g√©n√©ralit√©s, les formules vagues ou les r√©ponses hors sujet."
    "- Gardes en t√™te que tu dois toujours envisager ta r√©ponse dans le contexte de l'insertion socio-professionnelle et de l'inclusion par l'activit√© √©conomique."
    "- Bases-toi en priorit√© sur les informations pr√©sentes dans le contexte."
    "- Quand tu cites une information, indique l‚Äôidentifiant du document (ex. `[Doc3]`, `[Doc7]`).  "
    "- Si une partie de la r√©ponse demand√©e n‚Äôest pas couverte par le contexte, indique clairement : ¬´ Je n‚Äôai pas trouv√© d‚Äôinformation dans les documents fournis concernant ‚Ä¶ ¬ª.  "
    "- Si tu peux proposer une piste ou question compl√©mentaire (sans l‚Äôimposer), tu peux l‚Äôajouter √† la fin (en pr√©cisant que c‚Äôest une suggestion)."
    "\n\n"
    f"R√©pond maintenant √† la question :  \n**{query}**"
)

```

### 3. RAG

La derni√®re √©tape consiste √† ex√©cuter la g√©n√©ration de la r√©ponse √† proprement parler.
C‚Äôest ici que s‚Äôeffectue la mise en ≈ìuvre du m√©canisme de RAG (Retrieval-Augmented Generation) : le mod√®le est aliment√© √† la fois par les instructions et par le contexte documentaire issu de la recherche vectorielle.

Le code pr√©sent√© illustre un appel asynchrone √† l‚ÄôAPI `/api/chat` d‚ÄôOllama, qui permet de dialoguer avec un mod√®le local open-weight (ici `gpt-oss:20b`).
Le flux de sortie est trait√© en continu (*streaming*), ce qui offre une meilleure r√©activit√© et permet d‚Äôafficher la r√©ponse au fur et √† mesure de sa g√©n√©ration.
En cas d‚Äôerreur r√©seau ou de r√©ponse vide, des exceptions explicites sont lev√©es afin de faciliter le diagnostic.

Cette approche simple mais robuste constitue un prototype minimal fonctionnel de pipeline RAG :

1. r√©cup√©ration du contexte pertinent ;
2. pr√©paration des prompts ;
3. appel du mod√®le ;
4. restitution d‚Äôune r√©ponse sourc√©e et structur√©e.

Bien que ce code soit volontairement exp√©rimental et non industrialis√©, il d√©montre la faisabilit√© d‚Äôun chatbot m√©tier reposant exclusivement sur des mod√®les open weight et des technologies ouvertes, garantissant la ma√Ætrise totale des donn√©es et du comportement du mod√®le.

```python
# Appel de l'API /generate

try:
    answer = await request_ollama_chat(
        [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]
    )
except LLMServiceError as exc:
    raise AnswerGenerationError(str(exc)) from exc

return AskResponse(answer=answer, documents=documents)

```

```python
# D√©finition de la m√©thode pour appeler l'API /api/chat de Ollama

async def request_ollama_chat(messages: Iterable[Mapping[str, str]]) -> str:
    """Call the Ollama chat endpoint and return the assistant content."""

    payload = {"model": settings.ollama_model, "messages": list(messages)}

    chunks: list[str] = []

    try:
        async with httpx.AsyncClient(timeout=settings.ollama_timeout_seconds) as client:
            async with client.stream(
                "POST",
                f"{settings.ollama_base_url}/api/chat",
                json=payload,
            ) as response:
                response.raise_for_status()

                async for line in response.aiter_lines():
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                    except json.JSONDecodeError:
                        # Ignore malformed chunks, continue reading stream.
                        continue

                    message = data.get("message") if isinstance(data, dict) else None
                    if isinstance(message, dict):
                        content_piece = message.get("content")
                        if isinstance(content_piece, str):
                            chunks.append(content_piece)

                    if data.get("done") is True:
                        break
    except httpx.HTTPStatusError as exc:
        detail = exc.response.text
        raise LLMServiceError(
            f"LLM request failed with status {exc.response.status_code}: {detail}"
        ) from exc
    except httpx.HTTPError as exc:
        raise LLMServiceError("Unable to contact LLM service") from exc

    content = "".join(chunks).strip()
    if not content:
        raise LLMServiceError("LLM response missing assistant content")

    return content

```

> ‚òùÔ∏è Le seul m√©rite du code ci-dessus est d'√™tre fonctionnel.
> Je l'ai √©crit dans le cadre d'un projet de R&D non-industrialis√© √† des fins de veille technologique et de satisfaction de ma curiosit√© personnelle.
> Il est tr√®s largement perfectible et am√©liorable et ne doit surtout pas √™tre d√©ploy√© en production en l'√©tat !

## IHM

Pour la partie IHM, je ne me suis pas trop pris la t√™te. 
J'ai demand√© √† ChatGPT / Codex de me g√©n√©rer une application Next.js de type Chatbot qui interroge l'API d√©velopp√©e plus haut.

Le code source est disponible depuis le sous-r√©pertoire [/web](https://github.com/jbuget/ia-custom-chatbot/tree/main/web).
Pas grand chose de notable ici.

![Arborescence de l'application web front-end en Next.js avec focus sur l'appel √† l'API](webapp.png)


## Conclusion

Nous arrivons au bout de cette aventure et de cet article (üòÆ‚Äçüí® ouf !).

Il y aurait √©norm√©ment de choses √† creuser et d'**am√©liorations √† apporter** pour aller plus loin :

* s√©curiser les endpoints, prompts, √©crans, points d'acc√®s / sortie
* tester diff√©rents algorythmes, que ce soit pour la vectorisation / indexation comme des donn√©es ou la g√©n√©ration de r√©ponse
* les prompts syst√®mes et utilisateurs peuvent aussi √™tre tr√®s sensiblement am√©lior√©s
* il faudrait tester et comprarer les r√©sultats / temps de r√©ponse avec des ressources GPU
* on pourrait aussi comparer avec des technologies telles que ChromaDB ou Pinecone
* en l'√©tat, l'UX n'est pas tr√®s r√©active, et le fait d'attendre la g√©n√©ration compl√®te de la r√©ponse ne donne pas "confiance" ou "envie". On pourrait mettre en place du Server-Side-Event ou des Web Sockets pour dynamiser le rendu
* telle qu'elle est impl√©ment√©e, l'application (front et back) ne tient pas compte des sessions de discussion. On pourrait supporter le multi-messages pour tenir de la m√©moire / contexte de toute la discussion pour g√©n√©rer une meilleure r√©ponse
* et plein d'autres choses encore‚Ä¶


Au-del√† du code, **j‚Äôai appris pas mal de choses essentielles** dans ce vaste far-west que repr√©sente l‚ÄôIA √† l‚Äôheure actuelle :

* Ce que j‚Äôai enfin compris : le principe d‚Äô*embedding* et la recherche vectorielle ‚Äî ce n‚Äôest pas de la magie, juste une autre fa√ßon d‚Äôexprimer la proximit√© entre les id√©es ;
* Ce que j‚Äôai aim√© explorer : diff√©rents runtimes et mod√®les open-source/weight (Ollama, vLLM, `gpt-oss:20b`, `nomic-embed-text-v2-moe`, etc.), chacun avec ses forces et ses limites ;
* Ce que j‚Äôai d√©couvert : qu‚ÄôOllama sait faire bien plus que de la g√©n√©ration de texte (notamment l‚Äôembedding), m√™me s‚Äôil reste parfois limit√© sur certains mod√®les r√©cents ;
* Ce qui m‚Äôa surpris : manipuler des frameworks comme PyTorch ou Sentence Transformers, c‚Äôest finalement assez abordable ;
* Et enfin : apr√®s dix ans de Java puis dix ans de JS/TS/Node.js, j‚Äôai vraiment pris plaisir √† mettre les mains dans Python (et ce n'√©tait pas gagn√© ü§£)

Prochaine √©tape (mais pas tout de suite ü´©) : pousser plus loin ce qu'il est possible de faire avec l'IA, notamment en jouant avec des ressources GPU.

Que tu sois un(e) humain(e) ou un bot, merci pour le courage et la patience de m'avoir lu jusque l√† ü§ó !