---
title: "Mesurer la valeur et l'impact d'un service num√©rique"
date: "2026-02-08T10:00:00+01:00"
draft: true
toc: true
categories: ["Produit", "Management", "Tech"]
keywords: ["m√©triques", "KPI", "impact", "produit", "3U", "North Star Metric", "vanity metrics", "Goodhart", "HEART", "AARRR", "product analytics", "SQL", "leading indicator", "lagging indicator"]
summary: "Retour d'exp√©rience sur la difficult√© de mesurer la valeur d'un service num√©rique, et pr√©sentation du mod√®le 3U (Utilisable, Utilis√©, Utile) ‚Äî avec des exemples concrets, des requ√™tes SQL et les pi√®ges √† √©viter."
description: "Comment structurer ses m√©triques produit avec le mod√®le 3U (Utilisable, Utilis√©, Utile), identifier sa North Star Metric, √©viter les vanity metrics et les biais de Goodhart ‚Äî illustr√© par un cas concret de gestion d'incidents avec des requ√™tes SQL."
---

Pendant pr√®s de 10 ans, j'ai travaill√© au sein de [beta.gouv.fr](https://beta.gouv.fr), l'incubateur de services num√©riques de l'√âtat ‚Äî chez [Pix](https://pix.fr) puis √† la [Plateforme de l'inclusion](https://inclusion.beta.gouv.fr). L'une des forces de beta.gouv, c'est d'imposer une discipline simple mais radicale : **piloter chaque produit par l'impact**. Concr√®tement, chaque service incub√© doit exposer une page `/stats` publique rendant compte de ses r√©sultats. Pas de m√©triques, pas de financement (*).

> (*) Pens√©e √©mue √† [Carnet de bord](https://beta.gouv.fr/startups/carnet.de.bord.html) ü™¶ et l'√©quipe : La√´titia, Vincent, Lionel, GUL et JOP üí™ ‚ù§Ô∏è

Je suis d'abord un ing√©nieur technique ‚Äî d√©veloppeur, tech lead, solution architect, CTO. Ce n'est pas forc√©ment le profil qu'on attend sur un sujet de m√©triques produit. Mais je suis convaincu que **les meilleurs d√©veloppeurs sont celles et ceux qui ont compris que la technique est au service du produit, pas l'inverse**. Comprendre le m√©tier, la strat√©gie, le "pourquoi" derri√®re chaque fonctionnalit√©, ce n'est pas sortir de son r√¥le ‚Äî c'est l'exercer pleinement. Et quand il s'agit de d√©finir les bonnes m√©triques, l'avis des techs compte : ce sont eux qui savent ce qui est mesurable, √† quel co√ªt, et avec quelle fiabilit√©.

Cette conviction m'a confront√© √† une r√©alit√© souvent sous-estim√©e : **d√©finir les bonnes m√©triques est extr√™mement difficile**. On confond vite les types de m√©triques, ce qu'on peut en d√©duire, ce qu'on doit en faire. On mesure le nombre de comptes cr√©√©s en pensant mesurer l'adoption. On suit le temps de r√©ponse du serveur en croyant mesurer la qualit√© du service. On affiche des courbes qui montent pour rassurer, sans jamais r√©pondre √† la vraie question : **est-ce que ce service est utile ?**

Au fil des projets, j'en suis venu √† structurer ma r√©flexion autour d'un cadre simple ‚Äî le **mod√®le 3U** :

- **Utilisable** ‚Üí est-ce que √ßa fonctionne ?
- **Utilis√©** ‚Üí est-ce que les gens s'en servent ?
- **Utile** ‚Üí est-ce que √ßa cr√©e de la valeur ?

Ce triptyque n'est pas une invention personnelle. La [r√®gle des 3U](https://lisio.fr/blog/article/la-regle-des-3u-utile-utilisable-utilise) ‚Äî Utile, Utilisable, Utilis√© ‚Äî est un principe bien √©tabli en UX et en conception produit, que l'on retrouve dans la communaut√© du [num√©rique responsable](https://fr.wiki.isit-europe.org/nr/Utile_Utilisable_Utilis%C3%A9), ou encore dans la [d√©marche agile](https://fr.linkedin.com/pulse/les-3u-utile-utilisable-utilis%C3%A9-et-sa-mise-en-via-lo%C3%AFc-albarracin). Ici, je l'emploie volontairement dans l'ordre **Utilisable ‚Üí Utilis√© ‚Üí Utile** pour piloter la cha√Æne causale op√©rationnelle, tout en gardant en t√™te que la finalit√© reste la **valeur cr√©√©e**. Ma contribution ici est de le **r√©interpr√©ter sous l'angle des m√©triques** ‚Äî en associant √† chaque dimension un type d'indicateur concret ‚Äî et de l'appliquer au pilotage par l'impact d'un service num√©rique.

Ces trois dimensions forment une cha√Æne de causalit√© : `Fonctionnement ‚Üí Usage ‚Üí Impact`. Un maillon faible compromet l'ensemble, mais cette s√©quence permet aussi de **diagnostiquer pr√©cis√©ment** o√π se situe un blocage ‚Äî technique, adoption ou cr√©ation de valeur. L'ordre de lecture sert l'analyse ; la priorit√© de d√©cision reste l'**impact**.

> Le mod√®le 3U s'inscrit dans la lign√©e du [HEART Framework](https://research.google/pubs/measuring-the-user-experience-on-a-large-scale-user-centered-metrics-for-web-applications/) (Google, 2010) et des [Pirate Metrics - AARRR](https://en.wikipedia.org/wiki/AARRR) (Dave McClure, 2007), en y ajoutant une dimension "Fonctionnement" critique pour les outils √† forte composante technique. En contexte B2B interne, l'adoption n'est pas virale mais **accompagn√©e**, et le ROI se mesure en **efficacit√© op√©rationnelle** plut√¥t qu'en chiffre d'affaires.
>
> *R√©f√©rences* : Rodden et al. (2010), McClure (2007), Croll & Yoskovitz (2013) - *Lean Analytics*

Dans la suite de cet article, je d√©taille chaque dimension, les leviers de valeur qu'elle recouvre, et les bonnes pratiques pour choisir ses indicateurs ‚Äî le tout illustr√© par un cas concret.


## Cas d'usage

Pour rendre tout √ßa concret, le plus simple est de s'appuyer sur un exemple. Pour la suite de l'article, nous allons prendre comme fil rouge un cas fictif : celui d'un **op√©rateur de bornes interactives** (billetterie, information, services) qui d√©ploie et maintient un parc de bornes chez des clients B2B et B2C.

Cet op√©rateur a d√©velopp√© en interne une **application de gestion d'incidents**. Elle :

- **d√©tecte les incidents** √† partir d'√©v√©nements remont√©s par diff√©rentes sources et syst√®mes tiers (t√©l√©m√©trie, supervision r√©seau, ticketing, GMAO, etc.) ;
- **fournit des outils de diagnostic et de suivi**, incluant des m√©canismes de r√©solution automatique s'appuyant entre autres sur l'IA ;
- **propose des tableaux de bord de pilotage et de supervision** permettant de conna√Ætre l'√©tat du parc en temps r√©el, avec des filtres par zone g√©ographique, client ou criticit√©.

C'est un bon terrain de jeu pour notre sujet : forte composante technique, utilisateurs m√©tier aux attentes concr√®tes, et n√©cessit√© de d√©montrer une valeur business tangible. Exactement le genre de service o√π la question "comment mesurer la valeur ?" se pose √† chaque comit√© de pilotage.


## Les trois types de m√©triques

### Fonctionnement ‚Üí *utilisable* : "Le syst√®me fait-il ce qu'il est cens√© faire ?"

Les m√©triques de fonctionnement mesurent la sant√© technique et l'int√©grit√© op√©rationnelle de la plateforme. Elles v√©rifient que les composants critiques ‚Äî interconnexions partenaires, automatisations, flux de donn√©es ‚Äî fonctionnent comme attendu. Ce sont les premi√®res √† surveiller : sans fiabilit√© technique, rien d'autre ne tient.

_Exemples :_

* Taux de liaison automatique avec le syst√®me de ticketing
* Nombre d'incidents avec synchronisation GMAO r√©ussie
* Taux de r√©solution automatique des d√©connexions < 2h

### Usage ‚Üí *utilis√©* : "Les utilisateurs se sont-ils appropri√© l'outil ?"

Une fois la fiabilit√© technique acquise, encore faut-il que les utilisateurs s'en servent. Les m√©triques d'usage quantifient l'adoption et les comportements : qui utilise le service, √† quelle fr√©quence, pour quelles actions. Elles r√©v√®lent si les workflows sont adopt√©s et permettent d'identifier les points de friction ou d'abandon.

_Exemples :_

* Nombre d'utilisateurs actifs (‚â• 1 action m√©tier sur 7j)
* Taux d'engagement (utilisateurs modifiant des statuts)
* Volume d'incidents trait√©s par semaine
* Taux d'utilisation d'une fonctionnalit√© cl√© (ex. diagnostic automatique)

### Impact ‚Üí *utile* : "Le produit apporte-t-il la valeur attendue ?"

Un service peut fonctionner parfaitement et √™tre largement utilis√© sans pour autant cr√©er de la valeur. Les m√©triques d'impact √©valuent la contribution concr√®te du service aux objectifs m√©tier de l'organisation : gains de temps, qualit√© de service, retour sur investissement.

_Exemples :_

* R√©duction du temps moyen de r√©solution des incidents
* Temps cumul√© gagn√© gr√¢ce au traitement via la plateforme
* Am√©lioration du taux de r√©solution au premier niveau (vs. escalade maintenance)
* Satisfaction utilisateurs sur les diagnostics propos√©s


## Les leviers de cr√©ation de valeur

La troisi√®me dimension ‚Äî l'impact ‚Äî m√©rite qu'on s'y attarde. Qu'est-ce que "cr√©er de la valeur", concr√®tement ? Au bout du compte, c'est presque toujours une question d'argent ‚Äî gagn√©, √©conomis√© ou non perdu. Mais les chemins pour y parvenir sont multiples.

**R√©duction des temps d'ex√©cution et d√©lais de traitement.** C'est le levier le plus visible : ce qui prenait des heures manuellement se fait en minutes. Le temps gagn√© se r√©investit dans des t√¢ches √† plus forte valeur ajout√©e.

**Capacit√© √† monter en √©chelle.** Qui dit plus vite, dit aussi plus. Le service permet d'adresser des volum√©tries et des probl√©matiques qu'on n'oserait pas ‚Äî ou ne pourrait pas ‚Äî g√©rer autrement. C'est un changement de nature, pas seulement de degr√©.

**R√©duction des co√ªts intrins√®ques.** Ce passage √† l'√©chelle ouvre un autre levier : la n√©gociation. En automatisant des traitements √† grande √©chelle, le service g√©n√®re des donn√©es de volum√©trie qui servent de base pour ren√©gocier les conditions aupr√®s des fournisseurs et prestataires.

**Fiabilisation et tra√ßabilit√© de la donn√©e.** R√©p√©ter et encha√Æner des t√¢ches minutieuses manipulant des montants ou des valeurs critiques conduit in√©vitablement √† une baisse d'attention, et par cons√©quent √† des erreurs de saisie ou de validation. Ces erreurs peuvent avoir des r√©percussions en cours ou en bout de cha√Æne ‚Äî logistique op√©rationnelle, ou pire, direction financi√®re. Automatiser des traitements avec des r√®gles claires, pr√©cises et document√©es limite fortement ces risques. Et l'automatisation apporte un b√©n√©fice suppl√©mentaire : chaque traitement laisse une trace. C'est critique en cas d'audit r√©glementaire ou de probl√®me de s√©curit√© ‚Äî c'est de la gestion de risque. Au-del√†, cet historique structur√© se r√©v√®le pr√©cieux pour des travaux prospectifs en Business Intelligence ou en IA.

**Am√©lioration de la reprise sur erreur.** Les dossiers √† la marge, les cas complexes, les exceptions : c'est l√† que les processus manuels √©chouent le plus souvent. Un service num√©rique bien con√ßu permet de d√©tecter, isoler et retraiter ces cas de mani√®re structur√©e.

**R√©duction de la p√©nibilit√© et am√©lioration de l'exp√©rience salari√©.** Au-del√† des syst√®mes, il y a les humains qui les utilisent. Des t√¢ches r√©p√©titives ou des conditions de travail m√©diocres usent les collaborateurs. Au mieux, ils quittent l'entreprise. Au pire, ils mettent leur talent et leur √©nergie au service de la concurrence. Or recruter, former et fid√©liser un collaborateur est long, co√ªteux et √©nergivore. La perte d'un collaborateur productif ‚Äî par son savoir, son savoir-faire, son implication ‚Äî peut rapidement devenir un co√ªt critique : mon√©taire, culturel et capacitaire. Un outil bien con√ßu contribue au plaisir de travail, √† la motivation et √† la fid√©lisation.

**Rayonnement interne et externe.** Enfin, un service qui fonctionne bien devient un atout de cr√©dibilit√© ‚Äî m√©tier, m√©thodologique, technique, culturel. Il attire les talents, inspire les √©quipes voisines et peut devenir un argument commercial ou partenarial.


## Choisir les bonnes p√©riodes d'analyse

Une fois les m√©triques identifi√©es, reste une question pratique : **sur quelle fen√™tre de temps les observer ?** Les p√©riodes pertinentes varient selon la nature de l'indicateur :

* 7 jours (semaine)
* 30 jours (mois)
* 90 jours (trimestre)
* 180 jours (semestre)
* 365 jours (ann√©e)

Mais toutes les m√©triques ne sont **pas pertinentes** sur toutes les p√©riodes. Un indicateur d'usage a du sens √† la semaine ; un indicateur de tendance structurelle n'en a qu'au trimestre ou au-del√† :

| M√©trique | 7j | 30j | 90j | 180j | 365j |
|---|---|---|---|---|---|
| **Utilisateurs actifs** | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ùå | ‚ùå |
| **Incidents trait√©s** | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è |
| **Temps moyen de r√©solution** | ‚ö†Ô∏è | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Taux de liaison ticketing** | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |

√Ä noter que la multiplication des p√©riodes engendre une forte augmentation de la complexit√© technique. Mieux vaut commencer avec deux ou trois fen√™tres bien choisies que de vouloir tout couvrir d'embl√©e.

Le choix de la p√©riode d√©pend aussi de la **nature pr√©dictive** de l'indicateur. En product analytics, on distingue les ***leading indicators*** (indicateurs avanc√©s) des ***lagging indicators*** (indicateurs retard√©s). Un *leading indicator* signale un probl√®me √† venir ‚Äî par exemple, une hausse des incidents stagnants annonce une saturation de l'√©quipe. Un *lagging indicator* constate un r√©sultat pass√© ‚Äî le temps moyen de r√©solution mesure une performance d√©j√† r√©alis√©e. Les premiers se lisent √† la semaine pour r√©agir vite ; les seconds prennent leur sens au mois ou au trimestre pour d√©gager des tendances.


## Des m√©triques qui √©voluent avec le produit

Il y a un pi√®ge plus insidieux que de mal mesurer : **continuer √† mesurer la bonne chose au mauvais moment**.

Au lancement d'un produit, suivre le nombre d'utilisateurs inscrits ou actifs est l√©gitime ‚Äî c'est m√™me vital. On a besoin de savoir si le service trouve son public, si l'adoption progresse, si les efforts de d√©ploiement portent leurs fruits. Mais pass√© un certain seuil ‚Äî une base d'utilisateurs stabilis√©e, une part de march√© atteinte, un usage int√©gr√© dans les habitudes ‚Äî ce m√™me indicateur perd sa valeur strat√©gique. Il continue de monter (ou de se maintenir), et c'est rassurant. Sauf que la criticit√© s'est d√©port√©e ailleurs.

C'est exactement ce qu'est une **[vanity metric](https://matthieu-sanogho.com/vanity-metrics-definition/)** : un indicateur qui donne l'illusion que tout va bien, sur lequel on se concentre par confort, mais qui ne guide plus aucune d√©cision. Le nombre de comptes cr√©√©s, le volume de pages vues, le total cumul√© de tickets trait√©s depuis le lancement ‚Äî autant de chiffres qui impressionnent dans un rapport, mais qui ne disent rien sur la valeur r√©ellement produite aujourd'hui.

Le mod√®le 3U aide √† s'en pr√©munir, √† condition d'accepter une r√®gle : **les m√©triques doivent √©voluer avec le produit**. √Ä chaque √©tape de maturit√©, la question dominante change :

- **Phase de lancement** ‚Üí Le syst√®me fonctionne-t-il ? *(focus Fonctionnement)*
- **Phase d'adoption** ‚Üí Les utilisateurs s'en servent-ils ? *(focus Usage)*
- **Phase de maturit√©** ‚Üí Le service cr√©e-t-il la valeur attendue ? *(focus Impact)*

Une m√©trique pertinente en phase d'adoption peut devenir vaniteuse en phase de maturit√©. Le nombre d'utilisateurs actifs, si critique au d√©marrage, finit par n'√™tre qu'un indicateur de maintenance une fois le service install√©. La vraie question devient alors : qu'est-ce que ces utilisateurs **accomplissent** gr√¢ce √† l'outil, et √† quel co√ªt ?

R√©√©valuer r√©guli√®rement ses m√©triques ‚Äî en supprimer, en ajouter, en faire √©voluer les seuils ‚Äî n'est pas un signe d'instabilit√©. C'est le signe d'un pilotage qui reste align√© avec la r√©alit√© du terrain.

## La North Star Metric

√Ä l'oppos√© des vanity metrics, il existe la notion de **[North Star Metric](https://amplitude.com/blog/product-north-star-metric)** (NSM) : l'indicateur unique qui capture le mieux la valeur que le produit cr√©e pour ses utilisateurs, et autour duquel toutes les autres m√©triques gravitent. Une bonne NSM a la particularit√© de "tirer" l'ensemble du produit : si elle progresse, c'est que tout le reste fonctionne.

Pour donner un exemple concret : du temps o√π je travaillais chez [Pix](https://pix.fr), ce que je consid√©rais √™tre la North Star Metric √©tait le **nombre de certifications d√©livr√©es**. Ce seul chiffre entra√Ænait tout le reste dans son sillage : la qualit√© du r√©f√©rentiel de questions, le fait que les utilisateurs devaient y r√©pondre pour se positionner dans la matrice de niveaux, la n√©cessit√© de fournir une exp√©rience d'accompagnement et de prescription adapt√©e, ainsi que les outils et moyens pour assurer la logistique de certification. Un indicateur, une cha√Æne de valeur compl√®te.

Pour reprendre notre fil rouge : dans le cas de la plateforme de gestion d'incidents pour bornes interactives, la North Star Metric serait le **taux de disponibilit√© op√©rationnelle du parc**. Ce seul chiffre entra√Æne toute la cha√Æne : pour qu'il progresse, il faut d√©tecter les incidents rapidement (fonctionnement), que les √©quipes les traitent efficacement (usage), et que les diagnostics automatiques soient pertinents (impact). Un indicateur unique, compr√©hensible de l'√©quipe technique au CODIR, et qui r√©sume l'essentiel : est-ce que les bornes marchent ?

Identifier sa NSM est un exercice difficile mais salutaire. Elle √©volue avec le produit ‚Äî et quand on n'arrive plus √† la formuler clairement, c'est souvent le signe qu'on a perdu de vue ce que le service est cens√© apporter.

## Quand la mesure corrompt l'objectif

M√™me avec les bonnes m√©triques, il reste un risque. Deux lois bien connues en sciences sociales mettent en garde contre ce qui se passe quand une mesure devient un objectif :

- La **[loi de Goodhart](https://en.wikipedia.org/wiki/Goodhart%27s_law)**, formul√©e par l'√©conomiste Charles Goodhart en 1975 et popularis√©e par l'anthropologue Marilyn Strathern en 1997 : ***"When a measure becomes a target, it ceases to be a good measure"*** ‚Äî quand une mesure devient un objectif, elle cesse d'√™tre une bonne mesure. Goodhart l'avait observ√© dans le domaine de la politique mon√©taire britannique ; Strathern l'a g√©n√©ralis√© √† tout syst√®me d'√©valuation.
- La **[loi de Campbell](https://en.wikipedia.org/wiki/Campbell%27s_law)**, √©nonc√©e par le psychologue Donald T. Campbell en 1979 : *"The more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor"* ‚Äî plus un indicateur quantitatif est utilis√© pour prendre des d√©cisions, plus il est soumis √† des pressions de corruption et plus il tend √† d√©former les processus qu'il est cens√© observer.

Autrement dit : d√®s qu'un indicateur sert √† √©valuer, r√©compenser ou sanctionner, les acteurs trouvent des moyens de le satisfaire sans n√©cessairement produire l'effet recherch√©. On optimise la m√©trique, pas l'objectif. Le nombre de tickets cl√¥tur√©s augmente ‚Äî mais les incidents complexes sont d√©coup√©s artificiellement. Le temps moyen de r√©solution baisse ‚Äî mais les cas difficiles sont requalifi√©s pour sortir des statistiques.

C'est pourquoi les m√©triques ne doivent jamais √™tre lues isol√©ment, et pourquoi la structure en trois dimensions du mod√®le 3U est un garde-fou : si l'usage progresse mais que l'impact stagne, c'est le signal qu'on optimise peut-√™tre le mauvais indicateur.


## En pratique : exemples de requ√™tes SQL

La th√©orie, c'est bien. Mais au quotidien, une m√©trique n'existe que si on sait l'extraire. Voici 3 requ√™tes SQL (PostgreSQL), une par dimension du mod√®le 3U, pour garder uniquement l'essentiel.

### Utilisable ‚Äî Fonctionnement : les incidents stagnants

Avant de parler d'impact, il faut s'assurer que l'op√©rationnel tourne. Cette requ√™te d√©tecte les incidents ouverts qui ne bougent plus depuis plus de 7 jours.

```sql
SELECT COUNT(*) AS stale_incidents
FROM incidents
WHERE status IN ('A traiter', 'En cours')
AND updated_at < NOW() - INTERVAL '7 days';
```

**Lecture.** Si ce volume monte durablement, le service n'est plus r√©ellement *utilisable* : la cha√Æne de traitement se bloque.

### Utilis√© ‚Äî Usage : les utilisateurs actifs (7j)

L'adoption r√©elle se mesure par les actions m√©tier, pas par le nombre de comptes cr√©√©s.

```sql
SELECT COUNT(DISTINCT user_id) AS active_users
FROM user_actions
WHERE action_type IN (
  'status_change',
  'comment',
  'ticketing_link',
  'diagnostic_feedback',
  'incident_detail_view'
)
AND performed_at > NOW() - INTERVAL '7 days';
```

**Lecture.** Une baisse des actifs signale souvent un probl√®me d'UX, de formation ou de pertinence fonctionnelle.

### Utile ‚Äî Impact : le taux de suivi des recommandations

Pour chaque incident d√©tect√©, le service permet de g√©n√©rer un diagnostic via IA, avec des suggestions d'actions de supervision ou de t√¢ches de maintenance √† effectuer.

La valeur n'existe que si ces recommandations changent effectivement les d√©cisions de terrain.

```sql
SELECT
  COUNT(*) AS total_proposed,
  COUNT(*) FILTER (WHERE status = 'completed') AS completed,
  ROUND(
    100.0 * COUNT(*) FILTER (WHERE status = 'completed') / COUNT(*),
  1) AS completion_rate_pct
FROM supervision_actions
WHERE created_at > NOW() - INTERVAL '7 days';
```

**Lecture.** Un taux √©lev√© indique un service r√©ellement *utile*. Un taux faible r√©v√®le un d√©calage entre les suggestions du produit et la r√©alit√© m√©tier.


## Ce que les chiffres ne disent pas

Les requ√™tes SQL pr√©sent√©es ci-dessus mesurent le *quoi* et le *combien*. Mais elles ne r√©pondent jamais au *pourquoi*.

Un taux de suivi des recommandations √† 80% est un excellent signal. Mais pourquoi les utilisateurs suivent-ils ces recommandations ? Par confiance dans l'outil ? Par obligation hi√©rarchique ? Par habitude ? La r√©ponse change radicalement l'interpr√©tation ‚Äî et les d√©cisions produit qui en d√©coulent.

C'est l√† qu'intervient le **qualitatif** : entretiens utilisateurs, observations terrain, verbatims collect√©s dans les tickets, enqu√™tes de satisfaction (NPS, SUS). Ces donn√©es ne se mettent pas dans un dashboard, mais elles sont souvent les plus √©clairantes. Quelques pratiques simples √† mettre en place :

- **Entretiens r√©guliers** (m√™me 15 min/mois) avec des utilisateurs repr√©sentatifs ‚Äî pas les power users, mais ceux qui peinent ou abandonnent.
- **Verbatims structur√©s** : collecter syst√©matiquement les retours libres (commentaires dans les tickets, messages Slack, demandes de support) et les cat√©goriser.
- **Enqu√™tes flash** : une question unique envoy√©e apr√®s une action cl√© ("Ce diagnostic vous a-t-il √©t√© utile ? Oui / Non / Partiellement"). Le taux de r√©ponse est faible, mais les r√©sultats sont exploitables.

**Le quantitatif dit *o√π* regarder. Le qualitatif dit *quoi* comprendre.** Les deux sont indispensables ‚Äî et le mod√®le 3U s'applique aussi bien √† l'un qu'√† l'autre : on peut recueillir du feedback qualitatif sur le fonctionnement ("l'interface rame le matin"), l'usage ("je n'utilise pas cette fonctionnalit√© parce que...") et l'impact ("gr√¢ce √† cet outil, j'ai pu traiter le double d'incidents").


## Exemples de pages `/stats` et bonnes pratiques

Si vous cherchez des r√©f√©rences concr√®tes, voici quelques pages utiles √† parcourir :

- [Indicateurs de beta.gouv.fr](https://beta.gouv.fr/stats) : un bon exemple de pilotage multi-niveaux (produits, standards, impact), avec publication publique assum√©e.
- [Mes Aides R√©no ‚Äî /stats](https://mesaidesreno.beta.gouv.fr/stats) : bon √©quilibre entre objectifs explicites, m√©triques d'usage et indicateurs de conversion.
- [France Chaleur Urbaine ‚Äî /stats](https://france-chaleur-urbaine.beta.gouv.fr/stats) : bon exemple de mise en avant d'indicateurs d'impact m√©tier lisibles (raccordements, CO2 √©vit√©, passage √† l'action).
- [Recommandations-collaboratives ‚Äî Statistiques d'impact](https://recommandations-collaboratives.beta.gouv.fr/stats-impact/) : int√©ressant pour la clart√© de la North Star Metric et le suivi du "passage √† l'action".

Ce qu'on retrouve dans les meilleures pages `/stats` :

- **Mettre en premier et en avant la valeur cr√©√©e (dimension "Utile").** La premi√®re information visible doit r√©pondre √† la question : "quelle valeur concr√®te est cr√©√©e ?"
- **Une m√©trique d'impact centrale (North Star) clairement formul√©e.** Le lecteur doit comprendre en 10 secondes ce que le produit am√©liore concr√®tement.
- **Un cha√Ænage explicite entre fonctionnement, usage et impact.** Montrer les 3U √©vite les lectures trompeuses ("beaucoup d'usage" sans valeur cr√©√©e, par exemple).
- **Des d√©finitions sans ambigu√Øt√©.** Pour chaque indicateur : p√©rim√®tre, formule, p√©riode d'observation, exclusions √©ventuelles.
- **Une date de fra√Æcheur visible.** Afficher "donn√©es au JJ/MM/AAAA" pour √©viter les interpr√©tations sur des chiffres obsol√®tes.
- **Des objectifs ou seuils de lecture.** Une courbe seule ne dit rien ; une cible ou un seuil d'alerte rend la m√©trique actionnable.
- **Un focus sur les d√©cisions, pas sur les vanity metrics.** Garder peu d'indicateurs, mais ceux qui d√©clenchent des arbitrages produit.
- **De la transparence sur les limites.** Biais connus, angles morts, hypoth√®ses de calcul : mieux vaut les expliciter que sur-vendre la pr√©cision.
- **Une page maintenable.** Si l'actualisation d√©pend d'un export manuel, la page mourra ; privil√©gier des calculs automatis√©s.

Quand j'ouvre une page `/stats`, je regarde d'abord la valeur produite, puis je descends vers l'usage et enfin le fonctionnement pour comprendre ce qui l'explique.

Une r√®gle simple pour d√©marrer : **1 m√©trique "Utilisable", 1 m√©trique "Utilis√©", 1 m√©trique "Utile", puis seulement ensuite enrichir**. Mais dans la page elle-m√™me, affichez d'abord la m√©trique de valeur.


## Le co√ªt de la mesure

Il y a un sujet rarement abord√© mais crucial quand on est tech : **mesurer a un prix**.

Chaque m√©trique suppose une cha√Æne compl√®te : instrumentation du code (√©v√©nements, logs, tracking), pipeline de donn√©es (extraction, transformation, stockage), visualisation (dashboards, alertes) et maintenance dans la dur√©e (mont√©es de version, √©volutions de sch√©ma, corrections de bugs silencieux). Sans compter le temps humain d'analyse et d'interpr√©tation.

Ce co√ªt n'est pas n√©gligeable. J'ai vu des √©quipes passer plus de temps √† maintenir leurs dashboards qu'√† exploiter ce qu'ils affichaient. Le risque est r√©el : on finit par mesurer ce qui est *facile* √† mesurer plut√¥t que ce qui est *utile* √† mesurer ‚Äî un pi√®ge qui ram√®ne directement aux vanity metrics.

Quelques principes pour garder le cap :

- **Commencer petit.** Trois √† cinq m√©triques bien choisies valent mieux que vingt indicateurs que personne ne regarde. On peut toujours en ajouter.
- **Automatiser t√¥t.** Une m√©trique qu'il faut extraire manuellement chaque semaine sera abandonn√©e au bout d'un mois. Si elle vaut la peine d'exister, elle vaut la peine d'√™tre automatis√©e.
- **Supprimer sans remords.** Si un indicateur n'a d√©clench√© aucune d√©cision en trois mois, il ne sert √† rien. Le retirer all√®ge la charge technique et clarifie la lecture.
- **Budg√©ter la mesure.** Comme n'importe quelle fonctionnalit√©, l'observabilit√© a un co√ªt de d√©veloppement et de maintenance. L'int√©grer au backlog produit, pas dans un coin.


## Conclusion

Depuis mes tout d√©buts dans l'informatique et mon stage de fin d'√©tudes en entreprise, on m'a r√©p√©t√© l'importance de s'int√©resser au m√©tier, au fonctionnel, √† la strat√©gie dans laquelle un produit s'inscrit. On m'a tr√®s t√¥t parl√© du [Golden Circle de Simon Sinek](https://simonsinek.com/golden-circle/) ‚Äî *Why, How, What* ‚Äî et de l'importance de commencer par le *Why*. Cette conviction ne m'a jamais quitt√©. Mais entre le principe et la pratique, il peut y avoir un foss√© important : savoir qu'il faut mesurer la valeur est une chose, savoir *comment* en est une autre.

Le mod√®le 3U ‚Äî **Utilisable, Utilis√©, Utile** ‚Äî n'est pas un framework de plus √† plaquer sur un dashboard. C'est une grille de lecture pour se poser les bonnes questions au bon moment. Est-ce que √ßa marche ? Est-ce que les gens s'en servent ? Est-ce que √ßa change quelque chose ? Trois questions simples, mais qui demandent une vraie discipline pour y r√©pondre honn√™tement.

Si vous ne retenez qu'une chose de cet article : **commencez par une m√©trique par dimension**. Une m√©trique de fonctionnement, une m√©trique d'usage, une m√©trique d'impact. Trois chiffres. Pas plus. Si ces trois chiffres racontent une histoire coh√©rente, vous √™tes sur la bonne voie. S'ils se contredisent, vous venez de trouver o√π creuser.
